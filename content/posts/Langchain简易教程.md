+++
date = '2025-06-05T15:26:20+08:00'
draft = false
title = 'LangChain ç®€æ˜“æ•™ç¨‹'
series = 	['å­¦ä¹ ç¬”è®°']
series_weight=1
showTableOfContents='article.showTableOfContents'

+++
# **LangChain ç®€æ˜“æ•™ç¨‹**  

## **1. LangChain æ ¸å¿ƒæ¦‚å¿µè§£æ**  

### 1.1 æ¡†æ¶å®šä½ä¸ä»·å€¼  

LangChain æ˜¯ä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨å¼€å‘è®¾è®¡çš„å…¨ç”Ÿå‘½å‘¨æœŸæ¡†æ¶ï¼Œé€šè¿‡æ¨¡å—åŒ–ç»„ä»¶é™ä½å¼€å‘é—¨æ§›ï¼Œå®ç°ä»åŸå‹è®¾è®¡åˆ°ç”Ÿäº§éƒ¨ç½²çš„å…¨æµç¨‹æ”¯æŒã€‚å…¶æ ¸å¿ƒä¼˜åŠ¿ä½“ç°åœ¨ï¼š  

- **å¼€å‘æ•ˆç‡**ï¼šæä¾›é¢„åˆ¶çš„é“¾æ¡ï¼ˆChainsï¼‰ã€ä»£ç†ï¼ˆAgentsï¼‰ç­‰ç»„ä»¶ï¼Œæ— éœ€ä»é›¶æ„å»ºå¤æ‚é€»è¾‘  
- **å¯è§‚æµ‹æ€§**ï¼šé€šè¿‡ LangSmith å®ç°å…¨æµç¨‹ç›‘æ§ã€è¯„ä¼°ä¸ä¼˜åŒ–  
- **å·¥ç¨‹åŒ–èƒ½åŠ›**ï¼šå€ŸåŠ© LangServe å°†æ¨¡å‹æœåŠ¡è½¬åŒ–ä¸ºæ ‡å‡† API  

### 1.2 ç”Ÿæ€ç»„ä»¶æ¶æ„  

![CleanShot 2025-06-20 at 15.28.41@2x](https://s2.loli.net/2025/06/20/HMUXy4AQKdi86rN.png)

æ ¸å¿ƒåº“ä½“ç³»ï¼š  

| ç»„ä»¶åç§°            | åŠŸèƒ½å®šä½                                         |
| ------------------- | ------------------------------------------------ |
| langchain-core      | å®šä¹‰åŸºç¡€æŠ½è±¡ï¼ˆå¦‚ LLM æ¥å£ã€LCEL è¡¨è¾¾å¼è¯­è¨€ï¼‰     |
| langchain-community | é›†æˆç¬¬ä¸‰æ–¹å·¥å…·ï¼ˆæ–‡æ¡£åŠ è½½ã€å‘é‡æ•°æ®åº“ç­‰ï¼‰         |
| langchain-openai    | å°è£… OpenAI å…¼å®¹æ¥å£ï¼Œæ”¯æŒé˜¿é‡Œäº‘ç™¾ç‚¼ç­‰ç¬¬ä¸‰æ–¹æœåŠ¡ |
| langchain           | åŒ…å«é“¾ã€ä»£ç†ã€æ£€ç´¢ç­–ç•¥ç­‰æ ¸å¿ƒåº”ç”¨é€»è¾‘             |
| langgraph           | æ”¯æŒå¤šè§’è‰²å¯¹è¯ç³»ç»Ÿçš„å›¾ç»“æ„å»ºæ¨¡                   |

ç”Ÿæ€å·¥å…·é“¾ï¼š  

- **LangSmith**ï¼šLLM åº”ç”¨è°ƒè¯•ã€æµ‹è¯•ä¸ç›‘æ§å¹³å°ï¼Œæ”¯æŒè¿½è¸ªæç¤ºè¯ã€è¯„ä¼°ç”Ÿæˆè´¨é‡  
- **LangServe**ï¼šè½»é‡åŒ–æœåŠ¡éƒ¨ç½²å·¥å…·ï¼Œå¯å°† Chain è½¬æ¢ä¸º REST API

### 1.3 å…³é”®ç»„ä»¶

**å…³é”®ç»„ä»¶è§£é‡Š:**

- Promptsï¼šPromptsç”¨æ¥ç®¡ç† LLM è¾“å…¥çš„å·¥å…·ï¼Œåœ¨ä» LLM è·å¾—æ‰€éœ€çš„è¾“å‡ºä¹‹å‰éœ€è¦å¯¹æç¤ºè¿›è¡Œç›¸å½“å¤šçš„è°ƒæ•´ï¼Œæœ€ç»ˆçš„Prompså¯ä»¥æ˜¯å•ä¸ªå¥å­æˆ–å¤šä¸ªå¥å­çš„ç»„åˆï¼Œå®ƒä»¬å¯ä»¥åŒ…å«å˜é‡å’Œæ¡ä»¶è¯­å¥ã€‚
- Chainsï¼šæ˜¯ä¸€ç§å°†LLMå’Œå…¶ä»–å¤šä¸ªç»„ä»¶è¿æ¥åœ¨ä¸€èµ·çš„å·¥å…·ï¼Œä»¥å®ç°å¤æ‚çš„ä»»åŠ¡ã€‚
- Agentsï¼šæ˜¯ä¸€ç§ä½¿ç”¨LLMåšå‡ºå†³ç­–çš„å·¥å…·ï¼Œå®ƒä»¬å¯ä»¥æ‰§è¡Œç‰¹å®šçš„ä»»åŠ¡å¹¶ç”Ÿæˆæ–‡æœ¬è¾“å‡ºã€‚Agentsé€šå¸¸ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šActionã€Observationå’ŒDecisionã€‚Actionæ˜¯ä»£ç†æ‰§è¡Œçš„æ“ä½œï¼ŒObservationæ˜¯ä»£ç†æ¥æ”¶åˆ°çš„ä¿¡æ¯ï¼ŒDecisionæ˜¯ä»£ç†åŸºäºActionå’ŒObservationåšå‡ºçš„å†³ç­–ã€‚
- Memoryï¼šæ˜¯ä¸€ç§ç”¨äºå­˜å‚¨æ•°æ®çš„å·¥å…·ï¼Œç”±äºLLM æ²¡æœ‰ä»»ä½•é•¿æœŸè®°å¿†ï¼Œå®ƒæœ‰åŠ©äºåœ¨å¤šæ¬¡è°ƒç”¨ä¹‹é—´ä¿æŒçŠ¶æ€ã€‚  


## **2. ç¯å¢ƒé…ç½®ä¸ä¾èµ–ç®¡ç†**  

### 2.1 åŸºç¡€å®‰è£…  

```bash
pip install langchain  # å®‰è£…æ ¸å¿ƒæ¡†æ¶åŠåŸºç¡€ä¾èµ–  
```

**æ³¨æ„**ï¼šé»˜è®¤å®‰è£…ä¸åŒ…å«ç¬¬ä¸‰æ–¹é›†æˆä¾èµ–ï¼Œéœ€æ ¹æ®åœºæ™¯é¢å¤–å®‰è£…ï¼š  

- å‘é‡æ•°æ®åº“ï¼š`pip install chroma-client langchain-chroma`  
- OpenAI å…¼å®¹æ¥å£ï¼š`pip install openai`  
- æ–‡æ¡£å¤„ç†ï¼š`pip install python-docx`  


### 2.2 æ ¸å¿ƒæ¨¡å—ç‹¬ç«‹å®‰è£…  

```bash
pip install langchain-core       # åŸºç¡€æŠ½è±¡ä¸è¡¨è¾¾å¼è¯­è¨€  
pip install langchain-community  # ç¬¬ä¸‰æ–¹é›†æˆç»„ä»¶  
pip install langgraph           # å¤šè§’è‰²å¯¹è¯å›¾å»ºæ¨¡  
```


## **3. å¿«é€Ÿå…¥é—¨**  

### 3.1 API é…ç½®ä¸æ¨¡å‹è°ƒç”¨  

é˜¿é‡Œäº‘ç™¾ç‚¼æ¥å£ç¤ºä¾‹ï¼š  

```python
import os
from openai import OpenAI

# æ–¹å¼ä¸€ï¼šç¯å¢ƒå˜é‡é…ç½®ï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰
os.environ['DASHSCOPE_API_KEY'] = '<your-api-key>'
os.environ['ALIYUN_BASE_URL'] = 'https://dashscope.aliyuncs.com/compatible-mode/v1'

# æ–¹å¼äºŒï¼šæ˜¾å¼ä¼ å‚ï¼ˆå¼€å‘æµ‹è¯•ç”¨ï¼‰
client = OpenAI(
    api_key='<your-api-key>',
    base_url=os.getenv("ALIYUN_BASE_URL")
)

# è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
completion = client.chat.completions.create(
    model="qwen-plus",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯è¥é”€é¢†åŸŸä¸“å®¶"},
        {"role": "user", "content": "å¦‚ä½•è®¾è®¡æ•°å­—åŒ–è¥é”€è¯æœ¯ï¼Ÿ"}
    ]
)
print(completion.choices[0].message.content)
```

åœ¨ä¸LLMäº¤äº’æ—¶ï¼Œæ¶ˆæ¯é€šå¸¸åŒ…å«ä¸åŒçš„è§’è‰²ï¼Œæ¯ä¸ªè§’è‰²æœ‰å…¶ç‰¹å®šçš„å«ä¹‰å’Œä½¿ç”¨åœºæ™¯ ï¼š

| è§’è‰²      | ç”¨é€”è¯´æ˜         | æœ€ä½³å®è·µç¤ºä¾‹                             |
| --------- | ---------------- | ---------------------------------------- |
| system    | è®¾å®šåŠ©æ‰‹è¡Œä¸ºåŸºçº¿ | "ä½ æ˜¯ç”µå•†å®¢æœï¼Œéœ€ç”¨ç®€æ´è¯æœ¯è§£ç­”å”®åé—®é¢˜" |
| user      | ç”¨æˆ·è¾“å…¥å†…å®¹     | "è¯·é—®è¿™æ¬¾äº§å“æ”¯æŒ7å¤©æ— ç†ç”±é€€è´§å—ï¼Ÿ"      |
| assistant | å†å²å›å¤è®°å½•     | ç”¨äºä¸Šä¸‹æ–‡è®°å¿†ï¼Œé¿å…é‡å¤æé—®             |
| tool      | å·¥å…·è°ƒç”¨ç»“æœ     | RAG æµç¨‹ä¸­æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ               |

åœ¨è°ƒç”¨LLMæ—¶ï¼Œå¯ä»¥è®¾ç½®å¤šç§å‚æ•°æ¥æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„ç‰¹æ€§ï¼Œå¸¸è§å‚æ•°è¯´æ˜ï¼š

| å‚æ•°å           | å½±å“ç»´åº¦     | æ¨èå–å€¼èŒƒå›´                | å…¸å‹åº”ç”¨åœºæ™¯                          |
| ---------------- | ------------ | --------------------------- | ------------------------------------- |
| temperature      | è¾“å‡ºéšæœºæ€§   | 0.0ï¼ˆç¡®å®šæ€§ï¼‰~1.0ï¼ˆåˆ›æ„æ€§ï¼‰ | ä»£ç ç”Ÿæˆç”¨0.1ï¼Œè¥é”€æ–‡æ¡ˆç”¨0.7          |
| max_tokens       | è¾“å‡ºé•¿åº¦é™åˆ¶ | â‰¤ æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦            | GPT-3.5 å»ºè®® â‰¤ 4000ï¼ŒQwen å»ºè®® â‰¤ 8000 |
| presence_penalty | é¿å…é‡å¤ç”¨è¯ | 0.0~1.0                     | é•¿æ–‡æœ¬ç”Ÿæˆæ—¶è®¾ä¸º0.5é˜²æ­¢å†…å®¹å †ç Œ       |
| streaming        | æµå¼è¾“å‡º     | True/False                  | å‰ç«¯å®æ—¶å±•ç¤ºæ—¶å¯ç”¨ï¼Œæå‡äº¤äº’ä½“éªŒ      |

ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£è·å–å¯è°ƒç”¨æ¨¡å‹åˆ—è¡¨:

```
import os
from openai import OpenAI
# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(
    api_key='<your-api-key>', # æ›¿æ¢ä¸ºä½ çš„ API Key
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)
# è·å–æ¨¡å‹åˆ—è¡¨ï¼ˆDashScope æ”¯æŒè¿™ä¸ª endpointï¼‰
models = client.models.list()
# æ‰“å°å¯ç”¨æ¨¡å‹åç§°
for model in models:
    print(model.id)
```

### 3.2 ChatOpenAI ç±»

`ChatOpenAI` æ˜¯ LangChain ä¸­æœ€å¸¸ç”¨çš„ç±»ä¹‹ä¸€ï¼Œç”¨äºè°ƒç”¨ OpenAI æˆ–å…¼å®¹ OpenAI åè®®çš„æœåŠ¡ï¼ˆå¦‚é˜¿é‡Œäº‘ DashScopeã€Moonshot ç­‰ï¼‰ã€‚

ç¤ºä¾‹åˆå§‹åŒ–ä»£ç  ï¼š

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7,
    max_tokens=256,
    verbose=True,
    openai_api_key='<your-api-key>',
    openai_api_base="https://api.openai.com/v1 "
)
```

å‚æ•°è¯´æ˜ ï¼š

| **å‚æ•°å**        | **ç±»å‹**           | **æè¿°**                                                     |
| ----------------- | ------------------ | ------------------------------------------------------------ |
| `model`           | `str`              | ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œå¦‚ `"gpt-3.5-turbo"`ã€`"deepseek-chat"` ç­‰ã€‚å¯¹äºç™¾ç‚¼ï¼Œå¯è®¾ç½®ä¸º `"qwen-max"`ã€`"qwen-turbo"` ç­‰ ã€‚ |
| `temperature`     | `float`            | æ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼Œå€¼è¶Šé«˜è¶Šéšæœºï¼ŒèŒƒå›´ `[0, 1]`ï¼Œæ¨è `0.7` å·¦å³ ã€‚ |
| `max_tokens`      | `int`              | æ§åˆ¶æ¨¡å‹ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡ ã€‚                             |
| `verbose`         | `bool`             | æ˜¯å¦æ‰“å°ä¸­é—´æ—¥å¿—ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰ ã€‚                            |
| `openai_api_key`  | `str`              | API å¯†é’¥ï¼Œç”¨äºè®¤è¯ ã€‚                                        |
| `openai_api_base` | `str`              | æ¨¡å‹æœåŠ¡åœ°å€ï¼Œå¦‚æœæ˜¯è‡ªå®šä¹‰æœåŠ¡ï¼ˆå¦‚é˜¿é‡Œäº‘ï¼‰ï¼Œè¦è®¾ç½®ä¸ºå¯¹åº” URL ã€‚ |
| `n`               | `int`              | ä¸€æ¬¡ç”Ÿæˆå¤šå°‘ä¸ªå€™é€‰å›å¤ï¼Œé»˜è®¤æ˜¯ `1` ã€‚                        |
| `streaming`       | `bool`             | æ˜¯å¦å¯ç”¨æµå¼è¾“å‡ºï¼ˆé€å­—è¿”å›ç»“æœï¼‰ ã€‚                          |
| `request_timeout` | `float` or `tuple` | è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œé˜²æ­¢å¡æ­» ã€‚                                    |

### 3.3 è®°å¿†æ¨¡å—

`ChatMessageHistory`æ˜¯ä¸€ä¸ªéå¸¸è½»é‡çš„ç”¨äºå­˜å–`HumanMessages/AIMessages`ç­‰æ¶ˆæ¯çš„å·¥å…·ç±»ã€‚

```python
from langchain.memory import ChatMessageHistory

history = ChatMessageHistory()

history.add_user_message("hi!")

history.add_ai_message("whats up?")

# [HumanMessage(content='hi!', additional_kwargs={}), AIMessage(content='whats up?', additional_kwargs={})]
print(history.messages)
```

`ConversationBufferMemory` æ˜¯ LangChain ä¸­æœ€åŸºç¡€çš„è®°å¿†æ¨¡å—ï¼Œå®ƒä¼šå°†æ‰€æœ‰å¯¹è¯å†å²ä¿å­˜åœ¨ä¸€ä¸ªç¼“å†²åŒºé‡Œ ã€‚

ç¤ºä¾‹åˆå§‹åŒ–ä»£ç  ï¼š

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history", # å­˜å‚¨å¯¹è¯å†å²çš„é”®å
    input_key="user_input", # ç”¨æˆ·è¾“å…¥çš„é”®å
    output_key="ai_response", # AI è¾“å‡ºçš„é”®å
    return_messages=False, # è¿”å›å­—ç¬¦ä¸²æ ¼å¼è€Œä¸æ˜¯ Message å¯¹è±¡åˆ—è¡¨
    human_prefix="User", # ç”¨æˆ·å‰ç¼€
    ai_prefix="AI" # AI å‰ç¼€
)
```

 ç¤ºä¾‹ä»£ç ï¼š

```python
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAI

template = """You are a chatbot having a conversation with a human.

{chat_history}
Human: {human_input}
Chatbot:"""

prompt = PromptTemplate(
    input_variables=["chat_history", "human_input"], template=template
)
memory = ConversationBufferMemory(memory_key="chat_history")

llm = ChatOpenAI(
    model="deepseek-r1", # ç™¾ç‚¼æ”¯æŒçš„æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ qwen-turbo æˆ– qwen-plus
    api_key="<your-api-key>",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    temperature=0.7,
    max_tokens=512
)
llm_chain = LLMChain(
    llm=llm,
    prompt=prompt,
    verbose=True,
    memory=memory,
)
# Hello there! How are you?
print(llm_chain.predict(human_input="Hi, my friend"))
```

![CleanShot 2025-06-19 at 16.25.36@2x](https://s2.loli.net/2025/06/20/XhljVWHaQdprFuk.png)

åœ¨Agentä¸­ä½¿ç”¨å†…å­˜

```python
from langchain.agents import AgentExecutor, Tool, ZeroShotAgent
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain_community.utilities import SerpAPIWrapper
from langchain_openai import ChatOpenAI

# å®šä¹‰Tool
# éœ€è¦å®šä¹‰ç¯å¢ƒå˜é‡ export GOOGLE_API_KEY="", åœ¨ç½‘ç«™ä¸Šæ³¨å†Œå¹¶ç”ŸæˆAPI Key: https://serpapi.com/searches

search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events",
    )
]

# å®šä¹‰Prompt
prefix = """Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:"""
suffix = """Begin!"

{chat_history}
Question: {input}
{agent_scratchpad}"""

prompt = ZeroShotAgent.create_prompt(
    tools,
    prefix=prefix,
    suffix=suffix,
    input_variables=["input", "chat_history", "agent_scratchpad"],
)

# å®šä¹‰Memory
memory = ConversationBufferMemory(memory_key="chat_history")
llm =ChatOpenAI(
    model="qwen-plus", # ç™¾ç‚¼æ”¯æŒçš„æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ qwen-turbo æˆ– qwen-plus
    api_key="<your-api-key>",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    temperature=0.7,
    max_tokens=512
)
# å®šä¹‰LLMChain
llm_chain = LLMChain(llm=llm, prompt=prompt)

# å®šä¹‰Agent
agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)
agent_chain = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True, memory=memory
)
agent_chain.run(input="How many people live in canada?")
```

![CleanShot 2025-06-19 at 16.52.23@2x](https://s2.loli.net/2025/06/20/4aE2Pu916FlWg8N.png)

### 3.4 æç¤ºæ¨¡ç‰ˆ

`PromptTemplate` ç”¨äºæ ¼å¼åŒ–å•ä¸ªå­—ç¬¦ä¸²ï¼Œé€šå¸¸ç”¨äºè¾ƒç®€å•çš„è¾“å…¥ ã€‚

å®šä¹‰æ–¹å¼ä¸€ ï¼š

```python
from langchain.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template("æ¨¡æ¿å­—ç¬¦ä¸²")
```

å®šä¹‰æ–¹å¼äºŒ ï¼š

```python
prompt = PromptTemplate(
    input_variables=["name", "topic"],
    template="ä½ å¥½ï¼Œ{name}ï¼Œè¯·è°ˆè°ˆä½ å¯¹ {topic} çš„çœ‹æ³•ã€‚",
)
print(prompt.format(name="å°æ˜", topic="äººå·¥æ™ºèƒ½"))
```

å‚æ•°è¯´æ˜ ï¼š

| **å‚æ•°å**          | **æè¿°**                             |
| ------------------- | ------------------------------------ |
| `template`          | åŸå§‹æç¤ºè¯æ¨¡æ¿ï¼ŒåŒ…å« `{å˜é‡}` å ä½ç¬¦ |
| `input_variables`   | æ‰€æœ‰æ¨¡æ¿ä¸­ä½¿ç”¨çš„å˜é‡å               |
| `partial_variables` | å¯é€‰ï¼Œé¢„è®¾éƒ¨åˆ†å˜é‡å€¼                 |
| `validate_template` | æ˜¯å¦éªŒè¯æ¨¡æ¿å˜é‡ä¸€è‡´æ€§               |

### 3.5 ç¼“å­˜

å¦‚æœå¤šæ¬¡è¯·æ±‚çš„è¿”å›ä¸€æ ·ï¼Œå°±å¯ä»¥è€ƒè™‘ä½¿ç”¨ç¼“å­˜ï¼Œä¸€æ–¹é¢å¯ä»¥å‡å°‘å¯¹APIè°ƒç”¨æ¬¡æ•°èŠ‚çœtokenæ¶ˆè€—ï¼Œä¸€æ–¹é¢å¯ä»¥åŠ å¿«åº”ç”¨ç¨‹åºçš„é€Ÿåº¦ã€‚

```python
  from langchain.cache import InMemoryCache
  import time
  import langchain
  from langchain.llms import OpenAI
  llm = OpenAI(model_name="text-davinci-002", n=2, best_of=2)
  langchain.llm_cache = InMemoryCache()
  s = time.perf_counter()
  llm("Tell me a joke")
  elapsed = time.perf_counter() - s
  # executed first in 2.18 seconds.
  print("\033[1m" + f"executed first in {elapsed:0.2f} seconds." + "\033[0m")
  llm("Tell me a joke")
  # executed second in 0.72 seconds.
  elapsed2 = time.perf_counter() - elapsed
  print("\033[1m" + f"executed second in {elapsed2:0.2f} seconds." + "\033[0m")
```

### 3.6 æµå¼è¾“å‡º

å¦‚æœéœ€è¦æµå¼è¾“å‡ºï¼Œä½¿ç”¨`chain.stream()`å³å¯ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ä½¿ç”¨å‰éœ€è¦ç¡®è®¤å…·ä½“çš„æŸä¸ª`Output Parser`æ˜¯å¦æ”¯æŒæµå¼è¾“å‡ºåŠŸèƒ½ã€‚

```python
# åˆ›å»ºModel
from langchain_openai import ChatOpenAI
model = ChatOpenAI()

# åˆ›å»ºoutput_parser(è¾“å‡º)
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
class MathProblem(BaseModel):
    question: str = Field(description="the question")
    answer: str = Field(description="the answer of question")
    steps: str = Field(description="the resolve steps of question")

output_parser = JsonOutputParser(pydantic_object=MathProblem)

# åˆ›å»ºprompt(è¾“å…¥)
from langchain.prompts import PromptTemplate
prompt = PromptTemplate(
    template="You are good at math, please answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
)

# åˆ›å»ºChainå¹¶é“¾å¼è°ƒç”¨
chain = prompt | model | output_parser
print(chain.invoke({"query": "1+1=?"}))

# ä½¿ç”¨æµå¼è¾“å‡º
for s in chain.stream({"query": "1+1=?"}): # <<------
    print(s)
```

### 3.7 Agent

å¾ˆå¤šæ—¶å€™æœ‰äº›åŠŸèƒ½æ˜¯å¯ä»¥å¤ç”¨çš„ã€‚ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬å¯ä»¥æŠŠåŸºäºLLMå®ç°çš„ä¸€ä¸ªåŠŸèƒ½æŠ½è±¡æˆä¸€ä¸ªå¯å¤ç”¨çš„æ¨¡å—ï¼Œæ²¡é”™ï¼Œå®ƒå°±æ˜¯Agent ï¼

![CleanShot 2025-06-20 at 15.29.03@2x](https://s2.loli.net/2025/06/20/5RG8dUq9SMwx3rB.png)



Agentçš„æ ¸å¿ƒæ€æƒ³æ˜¯åŸºäºLLMå¤§è¯­è¨€æ¨¡å‹åšä¸€ç³»åˆ—çš„æ“ä½œï¼Œå¹¶æŠŠè¿™ä¸€ç³»åˆ—æ“ä½œæŠ½è±¡æˆä¸€ä¸ªå¯å¤ç”¨çš„åŠŸèƒ½ï¼æ˜ç™½äº†è¿™ä¸ªï¼Œå°±ä¼šå¯¹åé¢Agentçš„ç†è§£æœ‰å¾ˆå¤§å¸®åŠ©ï¼Œè®©æˆ‘ä»¬æŠŠç»“æ„ç²¾ç®€ä¸ºä¸‹å›¾æ‰€ç¤º

- Planning ï¼šAgentçš„è§„åˆ’é˜¶æ®µæ¶‰åŠç¡®å®šå¦‚ä½•åˆ©ç”¨LLMå¤§è¯­è¨€æ¨¡å‹ä»¥åŠå…¶ä»–å·¥å…·æ¥å®Œæˆç‰¹å®šä»»åŠ¡ã€‚è¿™åŒ…æ‹¬ç¡®å®šæ‰€éœ€çš„è¾“å…¥å’Œè¾“å‡ºï¼Œä»¥åŠé€‰æ‹©é€‚å½“çš„å·¥å…·å’Œç­–ç•¥ã€‚
- Memory ï¼šåœ¨è®°å¿†é˜¶æ®µï¼ŒAgentéœ€è¦èƒ½å¤Ÿå­˜å‚¨å’Œè®¿é—®è¿‡å»çš„ä¿¡æ¯ï¼Œä»¥ä¾¿åœ¨å½“å‰ä»»åŠ¡ä¸­ä½¿ç”¨ã€‚è¿™åŒ…æ‹¬å¯¹è¿‡å»å¯¹è¯æˆ–äº¤äº’çš„è®°å¿†ï¼Œä»¥åŠå¯¹ç›¸å…³å®ä½“å’Œå…³ç³»çš„è®°å¿†ã€‚
- Tools ï¼šå·¥å…·æ˜¯Agentæ‰§è¡Œä»»åŠ¡æ‰€éœ€çš„å…·ä½“æ“ä½œã€‚è¿™å¯èƒ½æ¶‰åŠåˆ°æ‰§è¡Œæœç´¢ã€æ‰§è¡Œç‰¹å®šç¼–ç¨‹è¯­è¨€ä»£ç ã€æ‰§è¡Œæ•°æ®å¤„ç†ç­‰æ“ä½œã€‚è¿™äº›å·¥å…·å¯ä»¥æ˜¯é¢„å®šä¹‰çš„å‡½æ•°æˆ–APIå¦‚`search()`, `python_execute()`ç­‰
- Action ï¼šåœ¨æ‰§è¡Œé˜¶æ®µï¼ŒAgentåˆ©ç”¨é€‰æ‹©çš„å·¥å…·æ‰§è¡Œç‰¹å®šçš„åŠ¨ä½œï¼Œä»¥å®Œæˆè§„åˆ’é˜¶æ®µç¡®å®šçš„ä»»åŠ¡ã€‚è¿™å¯èƒ½åŒ…æ‹¬ç”Ÿæˆæ–‡æœ¬ã€æ‰§è¡Œè®¡ç®—ã€æ“ä½œæ•°æ®ç­‰ã€‚åŠ¨ä½œçš„æ‰§è¡Œé€šå¸¸æ˜¯åŸºäºè§„åˆ’é˜¶æ®µçš„å†³ç­–å’Œè®°å¿†é˜¶æ®µçš„ä¿¡æ¯ã€‚

![CleanShot 2025-06-20 at 15.29.24@2x](https://s2.loli.net/2025/06/20/nPGOfzR98TuYH5h.png)

Agentç±»å‹æŒ‰ç…§æ¨¡å‹ç±»å‹ã€æ˜¯å¦æ”¯æŒèŠå¤©å†å²ã€æ˜¯å¦æ”¯æŒå‡½æ•°å¹¶è¡Œè°ƒç”¨ç­‰ç»´åº¦çš„ä¸åŒï¼Œä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ç§ä¸åŒçš„Agentï¼Œæ›´å¤šå¯ä»¥å‚è€ƒ[agent_typesæ–‡æ¡£](https://python.langchain.com/docs/modules/agents/agent_types/) ï¼š

- `OpenAI functions` ï¼šåŸºäºOpenAI Functionçš„Agent
- `OpenAI tools` ï¼šåŸºäºOpenAI Toolçš„Agent
- `XML Agent` ï¼šæœ‰äº›LLMæ¨¡å‹å¾ˆé€‚åˆç¼–å†™å’Œç†è§£XMLï¼ˆæ¯”å¦‚Anthropicâ€™s Claudeï¼‰ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨XML Agent
- `JSON Chat Agent` ï¼šæœ‰äº›LLMæ¨¡å‹å¾ˆé€‚åˆç¼–å†™å’Œç†è§£JSONï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨JSON Agent
- `Structured chat Agent` ï¼šä½¿ç”¨ç»“æ„åŒ–çš„èŠå¤©Agentå¯ä»¥ä½¿ç”¨å¤šè¾“å…¥çš„å·¥å…·
- `ReAct Agent` ï¼šåŸºäº[ReAct ](https://react-lm.github.io/)é€»è¾‘çš„Agent

Agentå¯ä»¥ä½¿ç”¨æœç´¢å·¥å…·æ¥è·å–ç‰¹å®šä¸»é¢˜çš„ä¿¡æ¯ï¼Œä½¿ç”¨è¯­è¨€å¤„ç†å·¥å…·æ¥ç†è§£å’Œç”Ÿæˆæ–‡æœ¬ï¼Œä½¿ç”¨ç¼–ç¨‹æ‰§è¡Œå·¥å…·æ¥æ‰§è¡Œç‰¹å®šçš„ä»£ç ç­‰ã€‚è¿™äº›å·¥å…·å…è®¸Agentä»å¤–éƒ¨è·å–æ‰€éœ€çš„ä¿¡æ¯ï¼Œå¹¶å¯¹å¤–éƒ¨ç¯å¢ƒäº§ç”Ÿå½±å“ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹å®ƒçš„å·¥ä½œæµç¨‹å¦‚ä¸‹æ‰€ç¤º ï¼š

- ç”¨æˆ·å‘èµ·è¯·æ±‚ï¼ŒAgentæ¥æ”¶è¯·æ±‚
- Agentä¼šæŠŠ `System Text + User Text + Tools/Functions` ä¸€èµ·ä¼ é€’ç»™LLMï¼ˆå¦‚è°ƒç”¨ChatGPTæ¥å£ï¼‰
- ç”±äºLLMå‘ç°ä¼ é€’äº†Tools/Functionså‚æ•°ï¼Œæ‰€ä»¥é¦–æ¬¡LLMåªè¿”å›åº”è¯¥è°ƒç”¨çš„å‡½æ•°ï¼ˆå¦‚search_funcï¼‰
- Agentä¼šè‡ªå·±è°ƒç”¨å¯¹åº”çš„å‡½æ•°ï¼ˆå¦‚search_funcï¼‰å¹¶è·å–åˆ°å‡½æ•°çš„è¿”å›ç»“æœï¼ˆå¦‚search_resultï¼‰
- AgentæŠŠå‡½æ•°çš„è¿”å›ç»“æœå¹¶å…¥åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼Œæœ€åå†æŠŠ `System Text + User Text + search_result` ä¸€èµ·ä¼ é€’ç»™LLM
- LLMæŠŠç»“æœè¿”å›ç»™Agent
- Agentå†æŠŠç»“æœè¿”å›ç»™ç”¨æˆ·

![CleanShot 2025-06-20 at 15.29.39@2x](https://s2.loli.net/2025/06/20/4rVbtGnlfhZYJHu.png)

åœ¨Langchainä¸­ï¼ŒToolsæ˜¯ä¸€ä¸ªåœ¨æŠ½è±¡å±‚å®šä¹‰çš„ç±»ï¼Œå®ƒå…·å¤‡ä¸€äº›å¦‚`name/description/args_schema/func`ç­‰ä¹‹ç±»çš„åŸºç¡€å±æ€§ï¼Œä¹Ÿæ”¯æŒä½¿ç”¨`@tool`è‡ªå®šä¹‰Toolå·¥å…·ï¼Œæ›´å¤šè¯·å‚çœ‹[æºç ](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/tools.py)å’Œ[æ¥å£æ–‡æ¡£](https://api.python.langchain.com/en/latest/tools/langchain_core.tools.Tool.html#langchain_core.tools.Tool)ï¼ŒåŒæ—¶æ¡†æ¶å†…éƒ¨ä¹Ÿé›†æˆäº†å¾ˆå¤šå¼€ç®±å³ç”¨çš„[Tools](https://python.langchain.com/docs/integrations/tools)å’Œ[ToolKitså·¥å…·é›†](https://python.langchain.com/docs/integrations/toolkits)ã€‚

ä½¿ç”¨`@tool`æ³¨è§£è‡ªå®šä¹‰ä¸€ä¸ªToolå·¥å…·

```python
from langchain.tools import tool

@tool
def search(query: str) -> str:
    """Look up things online."""
    return "LangChain"

print(search)
```

### 3.8 Chains

![CleanShot 2025-06-19 at 17.13.16@2x](https://s2.loli.net/2025/06/20/uAkoFBhPng4JRbe.png)



### 3.9 Callback

![CleanShot 2025-06-19 at 17.24.44@2x](https://s2.loli.net/2025/06/20/SIWf4wlHDbpLCqT.png)



Langchainæä¾›äº†ä¸€ç³»åˆ—ç³»ç»Ÿçº§åˆ«çš„å›è°ƒå‡½æ•°ï¼Œä¹Ÿå°±æ˜¯åœ¨æ•´ä¸ªç”Ÿå‘½å‘¨æœŸå†…çš„Hooké’©å­ï¼Œä»¥ä¾¿äºç”¨æˆ·åœ¨åº”ç”¨å±‚åšæ—¥å¿—ã€ç›‘æ§ç­‰å…¶ä»–å¤„ç†ã€‚

```python
from langchain_core.callbacks import BaseCallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰å›è°ƒå¤„ç†å™¨
class MyLoggingCallbackHandler(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        """å½“ LLM å¼€å§‹æ—¶è°ƒç”¨ã€‚"""
        print(f"--- LLM å¼€å§‹ ---")
        print(f"Serialized: {serialized}")
        print(f"Prompts: {prompts}")

    def on_llm_end(self, response, **kwargs):
        """å½“ LLM ç»“æŸæ—¶è°ƒç”¨ã€‚"""
        print(f"--- LLM ç»“æŸ ---")
        print(f"Response: {response.generations[0][0].text}")

    def on_chain_start(self, serialized, inputs, **kwargs):
        """å½“ Chain å¼€å§‹æ—¶è°ƒç”¨ã€‚"""
        print(f"--- Chain '{serialized.get('name', 'Unnamed Chain')}' å¼€å§‹ ---")
        print(f"Inputs: {inputs}")

    def on_chain_end(self, outputs, **kwargs):
        """å½“ Chain ç»“æŸæ—¶è°ƒç”¨ã€‚"""
        print(f"--- Chain ç»“æŸ ---")
        print(f"Outputs: {outputs}")

# 2. åˆå§‹åŒ– LLM å’Œ Prompt Template
llm = ChatOpenAI(
    model="qwen-plus", # ç™¾ç‚¼æ”¯æŒçš„æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ qwen-turbo æˆ– qwen-plus
    api_key="<your-api-key>",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    temperature=0.7,
    max_tokens=512
)
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚"),
    ("user", "{question}")
])

# 3. åˆ›å»ºä¸€ä¸ªé“¾
chain = prompt | llm | StrOutputParser()

# 4. åœ¨ Chain è°ƒç”¨æ—¶ä¼ å…¥å›è°ƒ
print("\n--- ç¤ºä¾‹ 1: é€šè¿‡ invoke ä¼ å…¥å›è°ƒ ---")
response = chain.invoke(
    {"question": "è§£é‡Šä¸€ä¸‹å…‰åˆä½œç”¨ã€‚"},
    config={"callbacks": [MyLoggingCallbackHandler()]}
)
print(f"æœ€ç»ˆå“åº”: {response}")

# 5. ä¹Ÿå¯ä»¥åœ¨æ„å»º LLM æ—¶ä¼ å…¥å›è°ƒ (åªå¯¹è¯¥ LLM æœ‰æ•ˆ)
print("\n--- ç¤ºä¾‹ 2: åœ¨ LLM æ„é€ å‡½æ•°ä¸­ä¼ å…¥å›è°ƒ ---")
llm_with_callback = ChatOpenAI(
    temperature=0.7, 
    model="qwen-plus", # ç™¾ç‚¼æ”¯æŒçš„æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ qwen-turbo æˆ– qwen-plus
    api_key="<your-api-key>",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    callbacks=[MyLoggingCallbackHandler()])
chain_with_llm_callback = prompt | llm_with_callback | StrOutputParser()
response_2 = chain_with_llm_callback.invoke({"question": "è®²ä¸ªç¬‘è¯ã€‚"})
print(f"æœ€ç»ˆå“åº”: {response_2}")
```

### 3.10 LCEL

LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ä¸€ç§æ„å»ºå¤æ‚é“¾çš„ç®€ä¾¿æ–¹æ³•ï¼Œè¯­æ³•æ˜¯ä½¿ç”¨`|`æˆ–è¿ç®—ç¬¦è‡ªåŠ¨åˆ›å»ºChainåï¼Œå³å¯å®Œæˆé“¾å¼æ“ä½œã€‚è¿™åœ¨èƒŒåçš„åŸç†æ˜¯pythonçš„`__ror__`é­”æœ¯å‡½æ•°ï¼Œæ¯”å¦‚`chain = prompt | model`å°±ç›¸å½“äº`chain = prompt.__or__(model)`ã€‚

ä¸‹é¢çœ‹ä¸€ä¸ªç®€å•çš„LCELä»£ç ï¼ŒæŒ‰ç…§ä¼ ç»Ÿçš„æ–¹å¼åˆ›å»º`prompt/model/output_parser`ï¼Œç„¶åå†ä½¿ç”¨`|`æˆ–è¿ç®—ç¬¦åˆ›å»ºäº†ä¸€ä¸ªChainï¼Œå®ƒè‡ªåŠ¨æŠŠè¿™3ä¸ªç»„ä»¶é“¾æ¥åœ¨äº†ä¸€èµ·ï¼Œè¿™éƒ½æ˜¯åœ¨åº•å±‚å®ç°çš„ï¼Œå¯¹åº”ç”¨å±‚ååˆ†å‹å¥½ ï¼

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_template("tell me a short joke about {topic}")
model = ChatOpenAI()
output_parser = StrOutputParser()

chain = prompt | model | output_parser

print(chain.invoke({"topic": "math"}))
```

åœ¨LCELçš„åº•å±‚ï¼Œä¸»è¦æ˜¯å®ç°äº†ä¸€å¥—é€šç”¨çš„`Runnable`åè®®ï¼Œåªè¦å„ç±»ç»„ä»¶éµå¾ªå¹¶å®ç°æ­¤åè®®ï¼Œä¾¿å¯ä»¥è‡ªåŠ¨å®Œæˆé“¾å¼ç»„åˆå’Œè°ƒç”¨ã€‚

1. ç»Ÿä¸€çš„æ¥å£ ï¼šæ¯ä¸ªLCELå¯¹è±¡éƒ½å®ç°è¯¥Runnableæ¥å£ï¼Œè¯¥æ¥å£å®šä¹‰äº†ä¸€ç»„é€šç”¨çš„è°ƒç”¨æ–¹æ³•ï¼ˆinvokeã€batchã€streamã€ainvokeã€ â€¦ï¼‰ã€‚è¿™ä½¿å¾—LCELå¯¹è±¡é“¾ä¹Ÿå¯ä»¥è‡ªåŠ¨æ”¯æŒè¿™äº›è°ƒç”¨ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸ªLCELå¯¹è±¡é“¾æœ¬èº«å°±æ˜¯ä¸€ä¸ªLCELå¯¹è±¡ã€‚
2. ç»„åˆåŸè¯­ ï¼šLCELæä¾›äº†è®¸å¤šåŸè¯­ï¼ˆæ¯”å¦‚__ror__é­”æœ¯å‡½æ•°ï¼‰ï¼Œå¯ä»¥è½»æ¾ç»„åˆé“¾ã€å¹¶è¡ŒåŒ–ç»„ä»¶ã€æ·»åŠ åå¤‡ã€åŠ¨æ€é…ç½®é“¾å†…éƒ¨ç­‰ç­‰ã€‚



ç¤ºä¾‹ä»£ç ï¼š

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_template("tell me a short joke about {topic}")
model = ChatOpenAI(
    model="qwen-plus", # ç™¾ç‚¼æ”¯æŒçš„æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ qwen-turbo æˆ– qwen-plus
    api_key="<your-api-key>",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    temperature=0.7,
    max_tokens=512
)
output_parser = StrOutputParser()

chain = prompt | model | output_parser

# invoke: æ™®é€šè¾“å‡º
print(chain.invoke({"topic": "math"}))

# ainvoke: å¼‚æ­¥è¾“å‡º
chain.ainvoke({"topic": "math"})

# stream: æµå¼è¾“å‡º
for chunk in chain.stream({"topic": "math"}):
    print(chunk, end="", flush=True)

# Batch: æ‰¹é‡è¾“å…¥
print(chain.batch([{"topic": "math"}, {"topic": "English"}]))
```



## **4. AI è¥é”€å¤§æ¨¡å‹å®æˆ˜**

### 4.1 æ–‡æ¡£å¤„ç†ä¸å‘é‡åŒ–æµç¨‹  

#### 4.1.1 æ­¥éª¤1ï¼šåŠ è½½ä¸åˆ†å‰²æ–‡æ¡£  

```python
#åŠ è½½æ¨¡å‹
import os
from openai import OpenAI
os.environ["DASHSCOPE_API_KEY"]="<your-api-key>"
os.environ["ALIYUN_BASE_URL"]="https://dashscope.aliyuncs.com/compatible-mode/v1"

client = OpenAI(
    # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼API Keyå°†ä¸‹è¡Œæ›¿æ¢ä¸ºï¼šapi_key=
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url=os.getenv("ALIYUN_BASE_URL"),
)

# åŠ è½½æ–‡æ¡£
from langchain.document_loaders import Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List

# ä½¿ç”¨ Docx2txtLoader åŠ è½½æ–‡æ¡£
loader = Docx2txtLoader("database/ä¼ä¸šæ•°å­—åŒ–è½¬å‹è¥é”€è¯æœ¯-tips.docx")
docs = loader.load()

# åˆ†å‰²æ–‡æœ¬
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=len,
    is_separator_regex=False,
)
splits = text_splitter.split_documents(docs)

if splits:
    print("æˆåŠŸsplitsæ–‡æ¡£ã€‚")
else:
    print("splitsæ–‡æ¡£å¤±è´¥ã€‚")
```

**`Docx2txtLoader` ç±»**:

- **åŠŸèƒ½**: ç”¨äºåŠ è½½ `.docx` æ ¼å¼çš„ Word æ–‡æ¡£å†…å®¹ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º LangChain çš„ `Document` å¯¹è±¡åˆ—è¡¨ã€‚æ¯ä¸ªå¯¹è±¡åŒ…å«ä» Word æ–‡æ¡£ä¸­æå–çš„çº¯æ–‡æœ¬å†…å®¹ã€‚
- å¸¸ç”¨å‚æ•°:
  - `file_path`: è¦åŠ è½½çš„ `.docx` æ–‡ä»¶è·¯å¾„ï¼ˆå­—ç¬¦ä¸²ï¼Œå¿…å¡«ï¼‰ã€‚
  - `encoding`: æ–‡æœ¬ç¼–ç æ–¹å¼ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º `None`ï¼Œé€šå¸¸ä¸éœ€è¦è®¾ç½®ï¼‰ã€‚
- å¸¸ç”¨æ–¹æ³•:
  - `.load()`: è¯»å–æ–‡æ¡£å¹¶è¿”å›ä¸€ä¸ª `List[Document]`ï¼Œæ¯ä¸ª `Document` çš„ `page_content` å±æ€§åŒ…å«æå–å‡ºçš„æ–‡æœ¬å†…å®¹ã€‚

**`RecursiveCharacterTextSplitter` ç±»**:

- **åŠŸèƒ½**: å°†é•¿æ–‡æœ¬æŒ‰æŒ‡å®šå­—ç¬¦é€’å½’åˆ‡åˆ†ï¼Œç”Ÿæˆå¤šä¸ªè¾ƒå°çš„æ–‡æœ¬å—ï¼ˆchunksï¼‰ï¼Œé€‚åˆæ¨¡å‹è¾“å…¥ã€‚é€šè¿‡è®¾ç½®é‡å éƒ¨åˆ†ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚
- å¸¸ç”¨å‚æ•°:
  - `chunk_size`: æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰ã€‚
  - `chunk_overlap`: å—ä¸å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼Œç”¨äºä¿æŒä¸Šä¸‹æ–‡è¿ç»­ã€‚
  - `separators`: åˆ†éš”ç¬¦åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§ä¾æ¬¡å°è¯•åˆ‡åˆ†ï¼ˆå¦‚ `["\n\n", "\n", " ", ""]`ï¼‰ã€‚
  - `length_function`: ç”¨äºè®¡ç®—æ–‡æœ¬é•¿åº¦çš„å‡½æ•°ï¼Œé»˜è®¤æ˜¯ `len`ã€‚
- å¸¸ç”¨æ–¹æ³•:
  - `.split_documents(documents)`: æ¥æ”¶ `List[Document]`ï¼Œè¿”å›åˆ†å‰²åçš„ `List[Document]`ã€‚
  - `.split_text(text)`: ç›´æ¥å¯¹å­—ç¬¦ä¸²è¿›è¡Œåˆ†å‰²ï¼Œè¿”å› `List[str]`ã€‚

#### 4.1.2 æ­¥éª¤2ï¼šæ–‡æœ¬å‘é‡åŒ–ä¸å‘é‡åº“æ„å»º  

ä¸ºäº†å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªåµŒå…¥æ¨¡å‹ã€‚è¿™é‡Œè‡ªå®šä¹‰ `AliyunEmbeddings` ç±»æ¥è°ƒç”¨ DashScope çš„åµŒå…¥æœåŠ¡ ï¼š

```python
from langchain.embeddings.base import Embeddings
from langchain.vectorstores import Chroma

# è‡ªå®šä¹‰é˜¿é‡Œäº‘åµŒå…¥æ¨¡å‹ï¼ˆéœ€å…ˆåˆå§‹åŒ– clientï¼‰
class AliyunEmbeddings(Embeddings):
    def embed_query(self, text):
        response = self.client.embeddings.create(
            input=text,
            model="text-embedding-v3"  # ç™¾ç‚¼æ–‡æœ¬åµŒå…¥æ¨¡å‹
        )
        return response.data[0].embedding
    
    def embed_documents(self, texts):
        return [self.embed_query(text) for text in texts]

# æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆé¦–æ¬¡è¿è¡Œæ—¶åˆ›å»ºï¼Œåç»­å¯åŠ è½½ï¼‰
embeddings = AliyunEmbeddings(client=client)
vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="docs/chroma/"  # æœ¬åœ°æŒä¹…åŒ–è·¯å¾„
)
```

**`langchain.embeddings.base.Embeddings` ç±»**:

- åŠŸèƒ½è¯´æ˜ï¼šEmbeddings æ˜¯ LangChain ä¸­å®šä¹‰åµŒå…¥æ¨¡å‹ï¼ˆæ–‡æœ¬å‘é‡åŒ–ï¼‰è¡Œä¸ºçš„åŸºç±»ã€‚å®ƒæ˜¯ä¸€ä¸ªæŠ½è±¡æ¥å£ï¼Œç”¨äºç»Ÿä¸€å„ç§åµŒå…¥æ¨¡å‹çš„è°ƒç”¨æ–¹å¼ã€‚æ‰€æœ‰å…·ä½“çš„åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ OpenAI çš„text-embedding-ada-002ã€ç™¾ç‚¼çš„ text-embedding-v3ã€HuggingFace çš„æœ¬åœ°æ¨¡å‹ç­‰ï¼‰éƒ½å¿…é¡»å®ç°è¿™ä¸ªæ¥å£ä¸­å®šä¹‰çš„ä¸¤ä¸ªæ ¸å¿ƒæ–¹æ³•ï¼š

  - `.embed_query()`: å¯¹å•ä¸ªå­—ç¬¦ä¸²è¿›è¡ŒåµŒå…¥ã€‚
  - `.embed_documents()`: å¯¹å¤šä¸ªå­—ç¬¦ä¸²åˆ—è¡¨è¿›è¡ŒåµŒå…¥ã€‚

- LangChain ä½¿ç”¨è¿™ä¸ªæ¥å£æ¥æ”¯æŒå¤šç§åµŒå…¥æ¨¡å‹ï¼Œå¹¶è®©å®ƒä»¬åœ¨å‘é‡æ•°æ®åº“ï¼ˆå¦‚ Chromaã€FAISS ç­‰ï¼‰ä¸­ç»Ÿä¸€ä½¿ç”¨ã€‚

**å‘é‡æ•°æ®åº“Chroma**ï¼š

Chroma æ˜¯ä¸€ä¸ªè½»é‡çº§ã€æœ¬åœ°è¿è¡Œçš„å‘é‡æ•°æ®åº“ï¼Œä¸»è¦ç”¨äºå­˜å‚¨æ–‡æ¡£åŠå…¶å¯¹åº”çš„å‘é‡è¡¨ç¤ºï¼ˆembeddingï¼‰ï¼Œæ”¯æŒæ ¹æ®è¯­ä¹‰å¿«é€Ÿæ£€ç´¢ç›¸ä¼¼å†…å®¹ã€‚å®ƒåœ¨ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿä¸­éå¸¸å¸¸ç”¨ï¼Œå¯ä»¥é«˜æ•ˆåœ°å¸®åŠ©å¤§æ¨¡å‹ä»çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯ ã€‚

æ„å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“ ï¼š

```python
from langchain.vectorstores import Chroma

persist_directory ="docs/chroma/"

# åˆå§‹åŒ–ä½ çš„ embedding ç±»
embeddings = AliyunEmbeddings(client=client, model="text-embedding-v3")

# æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
vectordb = Chroma.from_documents(
    documents=splits, # è¿™æ˜¯ä½ ä¹‹å‰åˆ†å‰²å¥½çš„ splits æ–‡æ¡£
    embedding=embeddings,
    persist_directory=persist_directory
)
print("å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆå¹¶ä¿å­˜è‡³ï¼š", persist_directory)

# æˆ–è€… åŠ è½½å·²æœ‰å‘é‡æ•°æ®åº“ï¼ˆåç»­è¿è¡Œæ—¶ï¼‰
# vectordb = Chroma(persist_directory=persist_directory,embedding_function=embeddings)
```

`Chroma` æ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬:

- `Chroma.from_documents()`: ä»æ–‡æ¡£æ„å»ºå‘é‡æ•°æ®åº“ã€‚
- `Chroma()`: åŠ è½½å·²æœ‰æ•°æ®åº“ã€‚
- `.similarity_search()`: æ ¹æ®é—®é¢˜æŸ¥æ‰¾æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚
- `.add_documents()`: å‘å·²æœ‰æ•°æ®åº“ä¸­æ·»åŠ æ–°æ–‡æ¡£ã€‚
- `.as_retriever()`: å°† Chroma å®ä¾‹å°è£…ä¸º LangChain çš„ Retrieverï¼Œç”¨äºé›†æˆåˆ°é“¾å¼æµç¨‹ä¸­ã€‚



### 4.2 RAG é—®ç­”é“¾å®ç°  

æ¥ä¸‹æ¥ï¼Œå®ç° RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æŸ¥è¯¢åŠŸèƒ½ï¼Œç»“åˆå‘é‡æ•°æ®åº“æ£€ç´¢å’Œ LLM ç”Ÿæˆ ï¼š

```python
#QAé—®ç­” - è¥é”€å’¨è¯¢ä¸“å®¶ç³»ç»Ÿ
from IPython.display import display, Markdown, clear_output
import time

def marketing_consulting(query):
    """æä¾›ä¸“ä¸šè¥é”€å»ºè®®"""
    try:
        # 1. ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = vectordb.similarity_search(query, k=3)
        # 2. æ„å»ºä¸Šä¸‹æ–‡å†…å®¹
        context = "\n\n".join([f"ğŸ“š æ¥æº {i+1}ï¼š{doc.page_content}" for i, doc in enumerate(retrieved_docs)])
        
        # 3. æ„é€ ç¬¦åˆQwenæ ¼å¼çš„messages
        messages = [
            {
                "role": "system",
                "content": (
                    "æ‚¨ç°åœ¨æ˜¯è¥é”€é¢†åŸŸé¦–å¸­ä¸“å®¶ï¼Œè¯·åŸºäºçŸ¥è¯†åº“å†…å®¹æä¾›ä¸“ä¸šå»ºè®®ã€‚è¦æ±‚ï¼š\n"
                    "1. åˆ†ç‚¹è¯´æ˜æ ¸å¿ƒç­–ç•¥\n"
                    "2. åŒ…å«å…·ä½“æ•°æ®æ”¯æ’‘\n"
                    "3. ç»™å‡ºå¯æ‰§è¡Œæ–¹æ¡ˆ\n"
                    "4. è¥é”€æœ¯è¯­ä¸“ä¸šå‡†ç¡®"
                )
            },
            {
                "role": "user",
                "content": f"è¥é”€ä¸»é¢˜ï¼š{query}\n\nå‚è€ƒèµ„æ–™ï¼š\n{context}"
            }
        ]
        
        # 4. è°ƒç”¨å¤§æ¨¡å‹è·å–ä¸“ä¸šå›ç­”
        response = client.chat.completions.create(
            model="qwen-turbo",  # ä½¿ç”¨æœ€æ–°æœ€å¼ºæ¨¡å‹
            messages=messages,
            temperature=0.3,   # é™ä½éšæœºæ€§ç¡®ä¿ä¸“ä¸šæ€§
            max_tokens=1500
        )
        answer = response.choices[0].message.content
        
        # 5. å¢å¼ºMarkdownæ˜¾ç¤ºæ•ˆæœ
        clear_output(wait=True)
        display(Markdown(f"### è¥é”€ä¸»é¢˜ï¼š**{query}**"))
        display(Markdown("### ä¸“ä¸šå»ºè®®"))
        display(Markdown(answer))
        
        return True
        
    except Exception as e:
        display(Markdown(f"**ç³»ç»Ÿé”™è¯¯**ï¼š`{str(e)}`"))
        return False

# ä¸»äº¤äº’å¾ªç¯
display(Markdown("## è¥é”€ä¸“å®¶å’¨è¯¢ç³»ç»Ÿå·²å¯åŠ¨"))
while True:
    user_query = input("\nè¯·è¾“å…¥è¥é”€ä¸»é¢˜ï¼ˆè¾“å…¥'é€€å‡º'ç»“æŸï¼‰ï¼š").strip()
    if user_query.lower() in ['é€€å‡º', 'exit', 'quit']:
        display(Markdown("## æ„Ÿè°¢ä½¿ç”¨è¥é”€ä¸“å®¶ç³»ç»Ÿï¼"))
        break
        
    if not user_query:
        display(Markdown("âš  **æç¤º**ï¼šè¯·è¾“å…¥æœ‰æ•ˆæŸ¥è¯¢å†…å®¹"))
        continue
        
    # æ·»åŠ æŸ¥è¯¢å¤„ç†åŠ¨ç”»
    display(Markdown(f"æ­£åœ¨åˆ†æï¼š**{user_query}** ..."))
    start_time = time.time()
    
    success = marketing_consulting(user_query)
    
    if success:
        elapsed = time.time() - start_time
        display(Markdown(f"â± åˆ†æè€—æ—¶ï¼š{elapsed:.1f}ç§’ | å­—ç¬¦æ•°ï¼š{len(user_query)}"))
```

## **5. Deepseek æ¨¡å‹å®æˆ˜**  

### 5.1 Ollama æœ¬åœ°æ¨¡å‹éƒ¨ç½²  

1. ç™»å½•[å®˜ç½‘](https://ollama.com/)ä¸‹è½½
2. æ‹‰å– DeepSeek æ¨¡å‹ 

```bash
ollama pull deepseek-r1:7b  # 70äº¿å‚æ•°ç‰ˆæœ¬ï¼Œé€‚åˆæœ¬åœ°æ¨ç†
```

### 5.2 LangChain é›†æˆ Ollama æ¨¡å‹  

**ChatOllama ç±»è¯¦è§£**

`ChatOllama` ç±»ç”¨äºè¿æ¥æœ¬åœ°é€šè¿‡ Ollama æ¡†æ¶è¿è¡Œçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¹¶è°ƒç”¨å…¶è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€å¯¹è¯äº¤äº’ç­‰ä»»åŠ¡ã€‚è¯¥ç±»å®ç°äº† LangChain çš„ `BaseChatModel` æ¥å£ï¼Œæ”¯æŒæ ‡å‡†çš„ LLM è°ƒç”¨æ–¹å¼ ã€‚

**å®‰è£…**:

```
!pip install -U langchain-ollama
```

**ä½¿ç”¨ç¤ºä¾‹**:

```Python
from langchain_community.chat_models import ChatOllama
ollama_llm = ChatOllama(model="deepseek-r1:7b")
```

**å‚æ•°è¯´æ˜**:

| **å‚æ•°å**    | **ç±»å‹** | **é»˜è®¤å€¼**   | **è¯´æ˜**                                      |
| ------------- | -------- | ------------ | --------------------------------------------- |
| `model`       | `str`    | `""llama2""` | ä½¿ç”¨çš„æ¨¡å‹åç§°åŠæ ‡ç­¾ï¼Œå¦‚ `""deepseek-r1:7b""` |
| `temperature` | `float`  | `0.8`        | æ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼ˆ`0`~`1`ï¼Œæ•°å€¼è¶Šä½å›ç­”è¶Šç¡®å®šï¼‰ |
| `max_tokens`  | `int`    | `None`       | æœ€å¤§è¾“å‡º token æ•°é‡é™åˆ¶                       |
| `top_p`       | `float`  | `0.9`        | Nucleus sampling å‚æ•°ï¼Œæ§åˆ¶é‡‡æ ·èŒƒå›´           |
| `streaming`   | `bool`   | `FALSE`      | æ˜¯å¦å¯ç”¨æµå¼è¾“å‡ºï¼ˆé€å­—ç”Ÿæˆï¼‰                  |

**å¸¸ç”¨æ–¹æ³•**ï¼š

| **æ–¹æ³•å**       | **åŠŸèƒ½è¯´æ˜**                                           |
| ---------------- | ------------------------------------------------------ |
| `.invoke(input)` | åŒæ­¥è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤ï¼Œè¾“å…¥ä¸ºå­—ç¬¦ä¸²æˆ–åŒ…å« prompt çš„å­—å…¸ |
| `.stream(input)` | æµå¼è°ƒç”¨æ¨¡å‹ï¼Œé€å­—è¿”å›è¾“å‡ºï¼ˆéœ€è®¾ç½® `streaming=True`ï¼‰  |
| `.batch(inputs)` | æ‰¹å¤„ç†å¤šä¸ªè¾“å…¥è¯·æ±‚                                     |
| `.generate()`    | ç”Ÿæˆå¤šä¸ªå›å¤å€™é€‰ï¼ˆå¯ç”¨äºé«˜çº§é‡‡æ ·ï¼‰                     |

### 5.3 æ„å»º FAISS å‘é‡åº“

![CleanShot 2025-06-19 at 17.24.44@2x](https://s2.loli.net/2025/06/20/SIWf4wlHDbpLCqT.png)

**OllamaEmbeddings ç±»è¯¦è§£**

`OllamaEmbeddings` ç±»ç”¨äºè°ƒç”¨ Ollama æä¾›çš„åµŒå…¥æ¨¡å‹ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼ˆembeddingï¼‰ï¼Œå¸¸ç”¨äºæ„å»ºå‘é‡æ•°æ®åº“ï¼ˆå¦‚ FAISSï¼‰ä»¥æ”¯æŒè¯­ä¹‰æ£€ç´¢ï¼ˆRAG æ¶æ„ä¸­çš„å…³é”®éƒ¨åˆ†ï¼‰ ã€‚

**å¸¸ç”¨å‚æ•°**:

| **å‚æ•°å**      | **ç±»å‹** | **é»˜è®¤å€¼**                    | **è¯´æ˜**                         |
| --------------- | -------- | ----------------------------- | -------------------------------- |
| `model`         | `str`    | `""nomic-embed-text:latestâ€"` | ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹åç§°               |
| `show_progress` | `bool`   | `False`                       | æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆé€‚ç”¨äºæ‰¹é‡åµŒå…¥ï¼‰ |

**å¸¸ç”¨æ–¹æ³•**:

| **æ–¹æ³•å**                | **åŠŸèƒ½è¯´æ˜**                                      |
| ------------------------- | ------------------------------------------------- |
| `.embed_query(text)`      | å°†å•ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²è½¬æ¢ä¸º embedding å‘é‡ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰ |
| `.embed_documents(texts)` | å°†å¤šä¸ªæ–‡æœ¬å­—ç¬¦ä¸²æ‰¹é‡è½¬æ¢ä¸º embedding å‘é‡åˆ—è¡¨     |

**FAISS**

FAISS æ˜¯ä¸€ä¸ªåŸºäº Facebook AI æä¾›çš„å‘é‡åº“ï¼Œå®ç°é«˜æ•ˆçš„ç›¸ä¼¼åº¦æ£€ç´¢ã€‚å®ƒæ”¯æŒå¿«é€Ÿè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼ˆANNï¼‰ä»¥åŠä¿å­˜å’ŒåŠ è½½æœ¬åœ°ç´¢å¼•ã€‚

æ„å»ºå‘é‡æ•°æ®åº“ ï¼š

```python
from langchain_huggingface import HuggingFaceEmbeddings

# åŠ è½½æœ¬åœ° Embedding æ¨¡å‹
embedding_model = HuggingFaceEmbeddings(
    model_name="bge-large-zh-v1.5", # æ”¹ä¸ºä½ æœ¬åœ°æ¨¡å‹è·¯å¾„
)

# ç¬¬å››æ­¥ï¼šåˆ›å»ºå‘é‡æ•°æ®åº“
from langchain_community.vectorstores import FAISS

vector_store = FAISS.from_documents(split_docs, embedding_model)
```

`FAISS` ä¸»è¦æ–¹æ³•:

- `from_documents`: ä»æ–‡æ¡£é›†åˆåˆ›å»º FAISS æ•°æ®åº“ã€‚
- `as_retriever()`: åˆ›å»ºä¸€ä¸ªæ£€ç´¢å™¨å¯¹è±¡ï¼Œç”¨äº RAG æµç¨‹ã€‚

### 5.4 æ„å»º PromptTemplate

æ„å»ºè‡ªå®šä¹‰çš„ PromptTemplate ç”¨äºé—®ç­”ç³»ç»Ÿ ï¼š

```python
#æ„å»ºpromptTemplate
from langchain_core.prompts import PromptTemplate
custom_prompt = PromptTemplate(
    template="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸šç»©æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ï¼šä¸Šä¸‹æ–‡ï¼š{context}é—®é¢˜ï¼š{question}è¯·ç”¨ä¸­æ–‡ç®€æ´æ˜äº†åœ°å›ç­”ï¼Œå¦‚æœæ— æ³•ä»æ•°æ®ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·è¯´æ˜ã€‚åŒæ—¶æ ¹æ®ç”¨æˆ·æå‡ºçš„é—®é¢˜å’ŒçŸ¥è¯†åº“ä¸­çš„å†…å®¹ç»™å‡ºä¸‰ä¸ªç”¨æˆ·æœ€å¯èƒ½å…³ç³»çš„é—®é¢˜ã€‚""",
    input_variables=["context","question"],
)
```

### 5.5 æ£€ç´¢é—®ç­”é“¾ï¼šRetrievalQA

`RetrievalQA` ç±»ç»“åˆæ£€ç´¢å™¨å’Œ LLM æ„å»ºé—®ç­”é“¾ï¼ˆRAGï¼‰ ã€‚

ä¸»è¦å‚æ•°:

- `llm`: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹ã€‚
- `chain_type`: æ§åˆ¶å¦‚ä½•å°†æ£€ç´¢ç»“æœä¼ é€’ç»™ LLMï¼Œå¸¸ç”¨å€¼åŒ…æ‹¬ `"stuff"`ã€`"map_reduce"`ã€`"refine"`ã€‚
- `retriever`: ç”¨æ¥ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆé€šå¸¸æ¥è‡ª vector storeï¼‰ã€‚
- `chain_type_kwargs`: ä¼ é€’ç»™åº•å±‚ Chain çš„å‚æ•°ï¼Œæ¯”å¦‚ promptã€‚
- `return_source_documents`: æ˜¯å¦è¿”å›æ£€ç´¢åˆ°çš„åŸå§‹æ–‡æ¡£ã€‚

`chain_type` å¯¹æ¯”:

| **åç§°**       | **ä½œç”¨**                                                     | **ç‰¹ç‚¹**                                                     |
| -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `"stuff"`      | å°†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ä¸€æ¬¡æ€§â€œå¡è¿›â€ prompt ä¸­ï¼Œä¾› LLM ä½¿ç”¨    | 1. æœ€ç®€å•ã€æœ€ç›´æ¥ã€‚2. é€‚åˆæ–‡æ¡£æ•°é‡å°‘ã€å†…å®¹çŸ­çš„æƒ…å†µã€‚ä¼˜ç‚¹ï¼šå“åº”å¿«ã€ä¸Šä¸‹æ–‡å®Œæ•´ã€‚ç¼ºç‚¹ï¼šå®¹æ˜“è¶… prompt ä¸Šé™ï¼ˆå¦‚ 4096 tokenï¼‰ã€‚ |
| `"map_reduce"` | å…ˆå¯¹æ¯ä¸ªæ–‡æ¡£å•ç‹¬ç”Ÿæˆç­”æ¡ˆï¼ˆmapï¼‰ï¼Œå†å°†å¤šä¸ªç­”æ¡ˆåˆå¹¶æˆæœ€ç»ˆç­”æ¡ˆï¼ˆreduceï¼‰ | 1. é€‚ç”¨äºæ–‡æ¡£å¤šæˆ–å†…å®¹é•¿çš„åœºæ™¯ã€‚2. å¯ä»¥é¿å… prompt è¶…é•¿é—®é¢˜ã€‚ä¼˜ç‚¹ï¼šå¯å¤„ç†å¤§é‡æ•°æ®ã€‚ç¼ºç‚¹ï¼šå¤šæ¬¡è°ƒç”¨ LLMï¼Œè¾ƒæ…¢ï¼›å¯èƒ½ä¸¢å¤±ä¸Šä¸‹æ–‡å…³è”ã€‚ |
| `"refine"`     | é€æ­¥ä¼˜åŒ–ç­”æ¡ˆï¼šå…ˆåŸºäºç¬¬ä¸€ä¸ªæ–‡æ¡£ç”Ÿæˆç­”æ¡ˆï¼Œç„¶åä¾æ¬¡ç”¨åç»­æ–‡æ¡£æ›´æ–°ç­”æ¡ˆ | 1. é€æ¡å¤„ç†æ–‡æ¡£ï¼ŒåŠ¨æ€æ›´æ–°ç­”æ¡ˆã€‚2. é€‚åˆä¿¡æ¯åˆ†æ•£ã€éœ€è¦ç»¼åˆåˆ¤æ–­çš„åœºæ™¯ã€‚ä¼˜ç‚¹ï¼šé€»è¾‘æ›´è¿è´¯ï¼Œç­”æ¡ˆè´¨é‡æ›´é«˜ã€‚ç¼ºç‚¹ï¼šè°ƒç”¨ LLM å¤šæ¬¡ï¼Œé€Ÿåº¦æ…¢ã€‚ |

åˆ›å»º RetrievalQA é“¾:

```python
#åˆ›å»ºretrievelQAé“¾
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=bailian_llm, # ä½¿ç”¨ç™¾ç‚¼ LLM
    chain_type="stuff",
    retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
    chain_type_kwargs={"prompt": custom_prompt},
    return_source_documents=True
)
```

**æ³¨æ„**ï¼š`bailian_llm` åœ¨æ­¤ç¤ºä¾‹ä¸­éœ€è¦é¢„å…ˆå®šä¹‰ï¼Œä¾‹å¦‚ï¼š

```python
#åŠ è½½æ¨¡å‹
from langchain_openai import ChatOpenAI
from openai import OpenAI

bailian_llm = ChatOpenAI(
    model="deepseek-r1", # ç™¾ç‚¼æ”¯æŒçš„æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ qwen-turbo æˆ– qwen-plus
    api_key="<your-api-key>",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    temperature=0.7,
    max_tokens=512
)
```

### 5.6 è¿è¡Œé—®ç­”ç³»ç»Ÿ

è¿è¡Œé—®ç­”ç³»ç»Ÿå¹¶æ ¼å¼åŒ–è¾“å‡ºç»“æœ ï¼š

```python
#è¿è¡Œé—®ç­”ç³»ç»Ÿ
def format_response(result):
    print("\nã€å›ç­”ã€‘")
    print(result["result"])
    print("\nã€å‚è€ƒæ¥æºã€‘")
    seen = set()
    for i, doc in enumerate(result["source_documents"][:3], 1):
        identifier = f"{doc.metadata['source']}-{doc.metadata.get('page','')}"
        if identifier not in seen:
            print(f"[æ¥æº{i}] {identifier}")
            seen.add(identifier)
while True:
    question = input("\nè¯·è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥qé€€å‡ºï¼‰ï¼š")
    if question.lower() =='q':
        break
    try:
        result = qa_chain.invoke({"query": question})
        format_response(result)
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
```

### 5.6 å®Œæ•´ä»£ç 

```python
from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# 1. æ–‡æ¡£å¤„ç†ï¼ˆåŒå‰æ–‡ï¼‰
loader = Docx2txtLoader("database/è¥é”€è¯æœ¯åº“.docx")
docs = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=40)
splits = text_splitter.split_documents(docs)

# 2. åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ BGE ä¸­æ–‡æ¨¡å‹ï¼‰
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-zh-v1.5",  # éœ€æå‰ä¸‹è½½åˆ°æœ¬åœ°
    model_kwargs={"device": "cpu"}  # å¯é€‰ "cuda" åŠ é€Ÿ
)

# 3. æ„å»º FAISS å‘é‡åº“ï¼ˆé€‚åˆå¤§è§„æ¨¡æ•°æ®ï¼‰
vector_store = FAISS.from_documents(splits, embeddings)

# 4. å®šä¹‰æç¤ºè¯æ¨¡æ¿
prompt = PromptTemplate(
    template="""
    ä½ æ˜¯ä¸šç»©æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š
    ä¸Šä¸‹æ–‡ï¼š{context}
    é—®é¢˜ï¼š{question}
    
    è¦æ±‚ï¼šç®€æ´å›ç­”ï¼Œè‹¥æ— æ³•å›ç­”è¯·è¯´æ˜ï¼Œå¹¶æ¨è3ä¸ªç›¸å…³é—®é¢˜ã€‚
    """,
    input_variables=["context", "question"]
)

# 5. åˆ›å»ºæ£€ç´¢é—®ç­”é“¾ï¼ˆä½¿ç”¨ refine ç­–ç•¥ä¼˜åŒ–ç­”æ¡ˆï¼‰
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="refine",
    retriever=vector_store.as_retriever(k=5),
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)

# 6. è¿è¡Œé—®ç­”ç³»ç»Ÿ
def format_answer(result):
    print("\nã€ç­”æ¡ˆã€‘", result["result"])
    print("\nã€å‚è€ƒæ¥æºã€‘", [doc.metadata["source"] for doc in result["source_documents"][:3]])

while True:
    query = input("è¯·è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰ï¼š")
    if query.lower() == 'q':
        break
    try:
        result = qa_chain.invoke({"query": query})
        format_answer(result)
    except Exception as e:
        print(f"é”™è¯¯ï¼š{str(e)}")
```



## 6. LangSmith

LangSmith æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºç”Ÿäº§çº§ LLM åº”ç”¨ç¨‹åºçš„å¹³å°ã€‚

å®ƒåŒ…å«è°ƒè¯•ã€æµ‹è¯•ã€è¯„ä¼°å’Œç›‘æ§åŸºäºä»»ä½• LLM æ¡†æ¶æ„å»ºçš„é“¾å’Œæ™ºèƒ½ä»£ç†ï¼Œå¹¶æ— ç¼é›†æˆ LangChainï¼ˆç”¨äºæ„å»º LLM çš„é¦–é€‰å¼€æºæ¡†æ¶ï¼‰ã€‚

### 6.1 æ–°å»ºé¡¹ç›®

- é…ç½®ç¯å¢ƒå˜é‡

```python
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT="https://api.smith.langchain.com"
LANGSMITH_API_KEY="<your-api-key>"
LANGSMITH_PROJECT="é—®ç­”æµ‹è¯•" #é¡¹ç›®å
OPENAI_API_KEY="<your-openai-api-key>"
```

- è¿è¡Œç¨‹åºå®Œæˆé¡¹ç›®åˆ›å»º
  - æ·»åŠ @traceable ä½¿å¾—langsmith å¯ä»¥è·Ÿè¸ªç¨‹åº

```python
from langchain_openai import ChatOpenAI
from zhipuai import ZhipuAI
from langsmith import traceable


zhipu_client = ZhipuAI(api_key="<your-api-key>")
@traceable
def glm():
    messages = [
        {
            "role": "system",
            "content": (
                "- Role: è¥é”€ç­–ç•¥é¡¾é—®\n"
                "- Background: ç”¨æˆ·éœ€è¦ä¸“ä¸šçš„è¥é”€å»ºè®®ï¼Œä»¥æå‡äº§å“æˆ–æœåŠ¡çš„å¸‚åœºè¡¨ç°ã€‚\n"
                "- Profile: ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„è¥é”€ä¸“å®¶ï¼Œå¯¹å¸‚åœºè¶‹åŠ¿ã€æ¶ˆè´¹è€…è¡Œä¸ºå’Œè¥é”€æ¸ é“æœ‰æ·±åˆ»çš„ç†è§£ã€‚\n"
                "- Skills: ä½ å…·å¤‡å¸‚åœºåˆ†æã€æ¶ˆè´¹è€…å¿ƒç†æ´å¯Ÿã€å“ç‰Œå»ºè®¾ã€æ•°å­—è¥é”€å’Œä¼ ç»Ÿè¥é”€çš„ç»¼åˆèƒ½åŠ›ã€‚\n"
                "- Goals: æä¾›é’ˆå¯¹æ€§çš„è¥é”€ç­–ç•¥ï¼Œå¸®åŠ©ç”¨æˆ·æé«˜å“ç‰ŒçŸ¥ååº¦ã€å¢åŠ å®¢æˆ·å‚ä¸åº¦å’Œæå‡é”€å”®ä¸šç»©ã€‚\n"
                "- Constrains: å»ºè®®åº”åŸºäºå¸‚åœºç ”ç©¶å’Œæ•°æ®åˆ†æï¼ŒåŒæ—¶è€ƒè™‘æˆæœ¬æ•ˆç›Šå’Œå¯æ‰§è¡Œæ€§ã€‚\n"
                "- OutputFormat: æä¾›å…·ä½“çš„è¥é”€ç­–ç•¥ã€æ‰§è¡Œæ­¥éª¤å’Œé¢„æœŸç»“æœçš„è¯¦ç»†æŠ¥å‘Šã€‚\n"
                "- Workflow:\n"
                "  1. äº†è§£ç”¨æˆ·çš„äº§å“æˆ–æœåŠ¡ç‰¹æ€§ï¼Œä»¥åŠç›®æ ‡å¸‚åœºå’Œå®¢æˆ·ç¾¤ä½“ã€‚\n"
                "  2. åˆ†æå¸‚åœºè¶‹åŠ¿å’Œç«äº‰å¯¹æ‰‹çš„è¥é”€æ´»åŠ¨ã€‚\n"
                "  3. æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚å’Œå¸‚åœºæƒ…å†µï¼Œåˆ¶å®šä¸ªæ€§åŒ–çš„è¥é”€ç­–ç•¥ã€‚\n"
                "  4. æä¾›æ‰§è¡Œç­–ç•¥çš„å…·ä½“æ­¥éª¤å’Œæ—¶é—´è¡¨ã€‚\n"
                "  5. é¢„æµ‹ç­–ç•¥çš„æ½œåœ¨æ•ˆæœï¼Œå¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚"
            )
        },
        {"role": "user", "content": "user_input"},
    ]


    response = zhipu_client.chat.completions.create(
        model="glm-4-flash",
        messages=messages,
    )
    return response

print(glm())
```

![CleanShot 2025-06-16 at 17.02.40@2x](https://s2.loli.net/2025/06/20/A4DJpxNiKezBGam.png)

### 6.2 å»ºç«‹æ•°æ®é›†

```Python
from langsmith import Client, wrappers
from openevals.llm import create_llm_as_judge
from openevals.prompts import CORRECTNESS_PROMPT
from openai import OpenAI

# Define the input and reference output pairs that you'll use to evaluate your app
client = Client()

# Create the dataset
dataset = client.create_dataset(
    dataset_name="Test", description="A sample dataset in LangSmith."
)

# Create examples in the dataset. Examples consist of inputs and reference outputs
examples = [
    {
        "inputs": {"question": "å¯¹ä¸šåŠ¡éœ€æ±‚è¿›è¡Œæ¸…æ´—ã€ç­›é€‰çš„æŒ‡æ ‡æœ‰å“ªäº›"},
        "outputs": {"answer": "â‘ éœ€æ±‚å®ç°çš„éš¾æ˜“åŠå¤æ‚ç¨‹åº¦ï¼›â‘¡éœ€æ±‚å®ç°çš„æ—¶é—´å‘¨æœŸï¼›â‘¢éœ€æ±‚å®ç°çš„æˆæœ¬ï¼›â‘£éœ€æ±‚çš„è½»é‡ç¼“."},
    },
    {
        "inputs": {"question": "åœ¨æ•°å­—åŒ–å»ºè®¾åˆæœŸä¼ ç»Ÿä¼ä¸šå­˜åœ¨å“ªäº›å›°æƒ‘ä¸æ‹…å¿ƒï¼Ÿ"},
        "outputs": {
            "answer": "â‘ æ‹…å¿ƒæ•°å­—åŒ–è½åœ°çš„æ•ˆæœ --- è½åœ°éš¾ï¼›â‘¡ä¼ä¸šç®¡ç†ç¼ºä¹æ ‡å‡†åŒ–çš„èƒ½åŠ› --- ç®¡ç†éš¾ï¼›â‘¢æ‹…å¿ƒå‘˜å·¥ç´ è´¨ä½æ— æ³•æ‰¿å—æ•°å­—åŒ–çš„ä¸“ä¸šæŠ€æœ¯èƒ½åŠ› ---æ¨å¹¿éš¾ï¼›â‘£æ‹…å¿ƒç³»ç»Ÿæ”¹å˜äº†åŸæœ‰çš„å·¥ä½œæ¨¡å¼ï¼Œåœ¨åº”ç”¨è¿‡ç¨‹ä¸­å—é˜» --- åº”ç”¨éš¾ï¼›â‘¤æ‹…å¿ƒæ•°å­—åŒ–ç³»ç»Ÿè¿‡äºå¤æ‚ã€ä¸“ä¸šï¼Œå‘˜å·¥éš¾ä»¥é€‚åº”ï¼Œæˆä¸ºå·¥ä½œè´Ÿæ‹…--- æ“ä½œéš¾ï¼›â‘¥ä¸çŸ¥é“å¦‚ä½•æ¨å¹¿æ•°å­—åŒ–ç³»ç»Ÿ --- è®¤çŸ¥éš¾ï¼›ä»¥ä¸Šæ˜¯å½“å‰éƒ¨åˆ†ä¼ ç»Ÿä¼ä¸šçš„å¸¸è§é—®é¢˜ï¼Œç©¶å…¶åŸå› æœ€ä¸»è¦çš„è¿˜æ˜¯å››ä¸ªâ€œç¼ºä¹â€ï¼šâ‘ ç¼ºä¹å¯¹æ•°å­—åŒ–çš„æ·±åº¦è®¤çŸ¥ï¼›â‘¡ç¼ºä¹è½¬å‹çš„é­„åŠ›ï¼›â‘¢ç¼ºä¹æ•°å­—åŒ–çš„ä¸“ä¸šé¢†å¯¼äººæ‰ï¼›â‘£ç¼ºä¹æ•°å­—åŒ–åŸºç¡€èƒ½åŠ›ï¼›"},
    },
    {
        "inputs": {"question": "å¦‚ä½•å¼€å±•æ•°å­—åŒ–å¯¹æ ‡å­¦ä¹ ï¼Ÿ"},
        "outputs": {
            "answer": "â‘ å¸¦ï¼šå¸¦ç›®çš„ã€å¸¦é—®é¢˜ã€å¸¦è¯šæ„ï¼›â‘¡çœ‹ï¼š ç¬¬ä¸€çœ‹,ç»„ç»‡ç®¡ç†èƒ½åŠ›ï¼›ç¬¬äºŒçœ‹ï¼ŒæŠ€æœ¯ä¸ä¸šåŠ¡çš„ååŒèƒ½åŠ›ï¼›ç¬¬ä¸‰çœ‹ï¼Œå¯¹ITçš„æŠ•å…¥æ”¯æŒèƒ½åŠ›ï¼›ç¬¬å››çœ‹ï¼Œå¯¹æ•°æ®çš„æ·±å…¥åº”ç”¨èƒ½åŠ›ï¼›ç¬¬äº”çœ‹ï¼Œè¸©äº†å¤šå°‘å‘ï¼Œè¶Ÿäº†å¤šå°‘é›·ï¼›â‘¢å­¦ï¼šå­¦å…¶æ–‡åŒ–ã€å­¦å…¶æ–¹æ³•ã€å­¦å…¶æªæ–½ï¼›â‘£å®šï¼šå®šç­å­ã€å®šå›¢é˜Ÿã€å®šè§„åˆ’ã€å®šè·¯çº¿ã€å®šæŠ•å…¥ã€å®šæ ‡å‡†ã€å®šè´£ä»»ã€å®šç»©æ•ˆï¼›"},
    },

]

# Add the examples to the dataset
client.create_examples(dataset_id=dataset.id, examples=examples)
```

![CleanShot 2025-06-16 at 17.03.17@2x](https://s2.loli.net/2025/06/20/icxw1JObeCdyq7u.png)



### 6.3 è¯„ä¼°é—®ç­”

#### 6.3.1 æ·»åŠ é…ç½®ä¿¡æ¯


```python
import os
os.environ['LANGCHAIN_TRACING_V2'] = 'true'
os.environ['LANGCHAIN_ENDPOINT']="https://api.smith.langchain.com"
os.environ['LANGCHAIN_API_KEY']="" # langsmithçš„api_key
os.environ['LANGCHAIN_PROJECT']="é—®ç­”æµ‹è¯•"
```

è·å–åˆ›å»ºçš„æ•°æ®é›†é“¾æ¥

![CleanShot 2025-06-16 at 16.58.56@2x](https://s2.loli.net/2025/06/20/FsW9xEYeH8pigJ1.png)


```python
from langsmith import evaluate, Client
from langsmith.schemas import Example, Run
import os
from zhipuai import ZhipuAI
os.environ['LANGCHAIN_API_KEY']=""# langsmithçš„api_key
client = Client()
dataset =client.clone_public_dataset("")#å¡«å…¥æ•°æ®é›†é“¾æ¥
zhipu_client = ZhipuAI(api_key="")#å¡«å…¥æ™ºè°±apikey
```

#### 6.3.2 å®šä¹‰è¯„ä¼°å™¨


```python
#æ ¹æ®ç”¨æˆ·è¾“å…¥ç»“åˆçŸ¥è¯†åº“ç”Ÿæˆç”Ÿæˆé’ˆå¯¹æ€§çš„è¥é”€ç­–ç•¥
def pipeline(user_input: str):
    messages = [
        {
            "role": "system",
            "content": (
                "- Role: è¥é”€ç­–ç•¥é¡¾é—®\n"
                "- Background: ç”¨æˆ·éœ€è¦ä¸“ä¸šçš„è¥é”€å»ºè®®ï¼Œä»¥æå‡äº§å“æˆ–æœåŠ¡çš„å¸‚åœºè¡¨ç°ã€‚\n"
                "- Profile: ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„è¥é”€ä¸“å®¶ï¼Œå¯¹å¸‚åœºè¶‹åŠ¿ã€æ¶ˆè´¹è€…è¡Œä¸ºå’Œè¥é”€æ¸ é“æœ‰æ·±åˆ»çš„ç†è§£ã€‚\n"
                "- Skills: ä½ å…·å¤‡å¸‚åœºåˆ†æã€æ¶ˆè´¹è€…å¿ƒç†æ´å¯Ÿã€å“ç‰Œå»ºè®¾ã€æ•°å­—è¥é”€å’Œä¼ ç»Ÿè¥é”€çš„ç»¼åˆèƒ½åŠ›ã€‚\n"
                "- Goals: æä¾›é’ˆå¯¹æ€§çš„è¥é”€ç­–ç•¥ï¼Œå¸®åŠ©ç”¨æˆ·æé«˜å“ç‰ŒçŸ¥ååº¦ã€å¢åŠ å®¢æˆ·å‚ä¸åº¦å’Œæå‡é”€å”®ä¸šç»©ã€‚\n"
                "- Constrains: å»ºè®®åº”åŸºäºå¸‚åœºç ”ç©¶å’Œæ•°æ®åˆ†æï¼ŒåŒæ—¶è€ƒè™‘æˆæœ¬æ•ˆç›Šå’Œå¯æ‰§è¡Œæ€§ã€‚\n"
                "- OutputFormat: æä¾›å…·ä½“çš„è¥é”€ç­–ç•¥ã€æ‰§è¡Œæ­¥éª¤å’Œé¢„æœŸç»“æœçš„è¯¦ç»†æŠ¥å‘Šã€‚\n"
                "- Workflow:\n"
                "  1. äº†è§£ç”¨æˆ·çš„äº§å“æˆ–æœåŠ¡ç‰¹æ€§ï¼Œä»¥åŠç›®æ ‡å¸‚åœºå’Œå®¢æˆ·ç¾¤ä½“ã€‚\n"
                "  2. åˆ†æå¸‚åœºè¶‹åŠ¿å’Œç«äº‰å¯¹æ‰‹çš„è¥é”€æ´»åŠ¨ã€‚\n"
                "  3. æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚å’Œå¸‚åœºæƒ…å†µï¼Œåˆ¶å®šä¸ªæ€§åŒ–çš„è¥é”€ç­–ç•¥ã€‚\n"
                "  4. æä¾›æ‰§è¡Œç­–ç•¥çš„å…·ä½“æ­¥éª¤å’Œæ—¶é—´è¡¨ã€‚\n"
                "  5. é¢„æµ‹ç­–ç•¥çš„æ½œåœ¨æ•ˆæœï¼Œå¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚"
            )
        },
        {"role": "user", "content": user_input},
    ]
    
    tools = [
        {
            "type": "retrieval",
            "retrieval": {
                "knowledge_id": "1854410905543143424",  # çŸ¥è¯†åº“ID
                "prompt_template": (
                    "ä»æ–‡æ¡£\n\"\"\"\n{{knowledge}}\n\"\"\"\nä¸­æ‰¾é—®é¢˜\n\"\"\"\n"
                    "{{question}}\n\"\"\"\nçš„ç­”æ¡ˆï¼Œæ‰¾åˆ°ç­”æ¡ˆå°±ä»…ä½¿ç”¨æ–‡æ¡£è¯­å¥å›ç­”é—®é¢˜ï¼Œå¹¶ä¸”å¯¹è¾“å‡ºæ ¼å¼è¿›è¡Œæ•´ç†ç¾åŒ–ï¼›"
                    "æ‰¾ä¸åˆ°ç­”æ¡ˆå°±ç”¨è‡ªèº«çŸ¥è¯†å›ç­”å¹¶ä¸”å‘Šè¯‰ç”¨æˆ·è¯¥ä¿¡æ¯ä¸æ˜¯æ¥è‡ªæ–‡æ¡£ã€‚\n"
                    "ä¸è¦å¤è¿°é—®é¢˜ï¼Œç›´æ¥å¼€å§‹å›ç­”ã€‚"
                ),
            },
        }
    ]
    
    response = zhipu_client.chat.completions.create(
        model="glm-4-flash",
        messages=messages,
        tools=tools,
    )
    return response.choices[0].message.content

# è¯„ä¼°ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ã€‚
def rateResult(generate: str, reference: str):
    messages = [
        {
            "role": "system",
            "content": (
                "- Role: æ–‡æœ¬ç›¸ä¼¼åº¦è¯„ä¼°ä¸“å®¶\n"
                "- Background: ç”¨æˆ·éœ€è¦å¯¹æ¯”ä¸¤ä¸ªæ–‡æ®µï¼Œå³â€œç­”æ¡ˆâ€å’Œâ€œå‚è€ƒå†…å®¹â€ï¼Œä»¥è¯„ä¼°å®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼åº¦å’Œå‡†ç¡®æ€§ã€‚\n"
                "- Profile: ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æœ¬åˆ†æä¸“å®¶ï¼Œæ“…é•¿é€šè¿‡æ¯”è¾ƒå’Œå¯¹ç…§ä¸åŒæ–‡æœ¬å†…å®¹ï¼Œå‡†ç¡®è¯„ä¼°å®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼åº¦å’Œä¸€è‡´æ€§ã€‚\n"
                "- Skills: ä½ å…·å¤‡æ–‡æœ¬è§£æã€è¯­ä¹‰ç†è§£ã€ä¿¡æ¯æ¯”å¯¹å’Œè¯„åˆ†ç³»ç»Ÿè®¾è®¡çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ ¹æ®é¢„è®¾æ ‡å‡†å¯¹æ–‡æœ¬ç›¸ä¼¼åº¦è¿›è¡Œé‡åŒ–è¯„ä¼°ã€‚\n"
                "- Goals: æ ¹æ®â€œç­”æ¡ˆâ€ä¸â€œå‚è€ƒå†…å®¹â€çš„ç›¸ä¼¼åº¦å’Œå‡†ç¡®æ€§ï¼Œç»™å‡º1è‡³10çš„é‡åŒ–å¾—åˆ†ã€‚\n"
                "- Constrains: è¯„ä¼°å¿…é¡»åŸºäºå®¢è§‚æ ‡å‡†ï¼Œç¡®ä¿è¯„åˆ†çš„å…¬æ­£æ€§å’Œä¸€è‡´æ€§ã€‚\n"
                "- OutputFormat: è¿”å›ä¸€ä¸ª1è‡³10çš„å¾—åˆ†ï¼Œä»£è¡¨â€œç­”æ¡ˆâ€ä¸â€œå‚è€ƒå†…å®¹â€çš„ç›¸ä¼¼åº¦ã€‚\n"
                "- Workflow:\n"
                "  1. ä»”ç»†é˜…è¯»å¹¶ç†è§£â€œç­”æ¡ˆâ€å’Œâ€œå‚è€ƒå†…å®¹â€ã€‚\n"
                "  2. æ¯”è¾ƒä¸¤ä¸ªæ–‡æ®µçš„ä¸»é¢˜ã€å…³é”®ä¿¡æ¯å’Œç»†èŠ‚æè¿°ã€‚\n"
                "  3. æ ¹æ®ç›¸ä¼¼åº¦è¯„åˆ†æ ‡å‡†ï¼Œç¡®å®šâ€œç­”æ¡ˆâ€ä¸â€œå‚è€ƒå†…å®¹â€çš„ç›¸ä¼¼åº¦å¾—åˆ†ã€‚\n"
                "  4. åªéœ€è¦è¾“å‡ºå¾—åˆ†ï¼Œä¸ç”¨è¾“å‡ºåˆ«çš„\n"
                "- Examples:\n"
                "  - ç­”æ¡ˆï¼šâ€œè‹¹æœæ˜¯ä¸€ç§çº¢è‰²çš„æ°´æœã€‚â€\n"
                "    å‚è€ƒå†…å®¹ï¼šâ€œè‹¹æœæ˜¯ä¸€ç§å¸¸è§çš„æ°´æœï¼Œé€šå¸¸å‘ˆçº¢è‰²æˆ–ç»¿è‰²ã€‚â€\n"
                "    7\n"
                "  - ç­”æ¡ˆï¼šâ€œæ°´çš„åˆ†å­å¼æ˜¯H2Oã€‚â€\n"
                "    å‚è€ƒå†…å®¹ï¼šâ€œæ°´çš„åˆ†å­å¼æ˜¯H2Oï¼Œæ˜¯æœ€ç®€å•çš„æ°§åŒ–ç‰©ä¹‹ä¸€ã€‚â€\n"
                "    7\n"
                "  - ç­”æ¡ˆï¼šâ€œåœ°çƒæ˜¯å¤ªé˜³ç³»çš„ç¬¬ä¸‰é¢—è¡Œæ˜Ÿã€‚â€\n"
                "    å‚è€ƒå†…å®¹ï¼šâ€œåœ°çƒæ˜¯å¤ªé˜³ç³»çš„ç¬¬ä¸‰é¢—è¡Œæ˜Ÿï¼Œä¹Ÿæ˜¯å”¯ä¸€å·²çŸ¥å­˜åœ¨ç”Ÿå‘½çš„è¡Œæ˜Ÿã€‚â€\n"
                "    7"
            )
        },
        {"role": "user", "content": "ç­”æ¡ˆæ˜¯ï¼š" + generate + "ï¼Œå‚è€ƒå†…å®¹æ˜¯" + reference},
    ]
    
    response = zhipu_client.chat.completions.create(
        model="glm-4-flash",
        messages=messages,
    )
    return response.choices[0].message.content


# Define an evaluator
def is_concise_enough(root_run: Run, example: Example) -> dict:
    score = rateResult(root_run.outputs["output"], example.outputs["answer"])
    return {"key": "is_concise", "score": int(score)}


```

#### 6.3.3 è¿è¡Œè¯„ä¼°


```python
result=evaluate(
lambda x: pipeline(x["question"]),
data=dataset.name,
evaluators=[is_concise_enough],
experiment_prefix="my experiment"
)
result
```



