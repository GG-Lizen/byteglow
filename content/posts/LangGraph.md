+++
date = '2025-06-20T16:02:13+08:00'
draft = true
title = 'LangGraph'
series =[ 'å­¦ä¹ ç¬”è®°']
series_weight=4
showTableOfContents='article.showTableOfContents'

+++

# LangGraph

LangGraphï¼Œå®ƒæ˜¯ä¸€ä¸ªPythonåº“ ã€‚å…¶ä½œç”¨æ˜¯æ„å»ºæœ‰çŠ¶æ€ã€å¤šæ“ä½œçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨ç¨‹åºï¼Œç”¨äºåˆ›å»ºæ™ºèƒ½ä½“ï¼ˆagentï¼‰å’Œç»„åˆæ™ºèƒ½ä½“ï¼ˆmulti-agentï¼‰æµç¨‹ã€‚ ä¸å…¶ä»–LLMåº”ç”¨æ¡†æ¶ç›¸æ¯”ï¼ŒLangGraphæœ‰æ ¸å¿ƒä¼˜åŠ¿ï¼š

- æŒä¹…æ‰§è¡Œï¼šæ„å»ºèƒ½å¤Ÿåœ¨å‡ºç°æ•…éšœæ—¶æŒç»­è¿è¡Œå¹¶é•¿æ—¶é—´å·¥ä½œçš„æ™ºèƒ½ä½“ï¼Œå¯è‡ªåŠ¨ä»åœæ­¢çš„ä½ç½®ç²¾ç¡®æ¢å¤è¿è¡Œã€‚
-  äººå·¥ä»‹å…¥ï¼šé€šè¿‡åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ä»»ä½•æ—¶åˆ»æ£€æŸ¥å’Œä¿®æ”¹æ™ºèƒ½ä½“çŠ¶æ€ï¼Œæ— ç¼èå…¥äººå·¥ç›‘ç£ã€‚
-  å…¨é¢è®°å¿†ï¼šåˆ›å»ºçœŸæ­£æœ‰çŠ¶æ€çš„æ™ºèƒ½ä½“ï¼Œå…·å¤‡ç”¨äºæŒç»­æ¨ç†çš„çŸ­æœŸå·¥ä½œè®°å¿†å’Œè·¨ä¼šè¯çš„é•¿æœŸæŒä¹…è®°å¿†ã€‚ 
- ä½¿ç”¨LangSmithè¿›è¡Œè°ƒè¯•ï¼šå€ŸåŠ©å¯è§†åŒ–å·¥å…·æ·±å…¥äº†è§£å¤æ‚çš„æ™ºèƒ½ä½“è¡Œä¸ºï¼Œè¿™äº›å·¥å…·å¯è¿½è¸ªæ‰§è¡Œè·¯å¾„ã€æ•æ‰çŠ¶æ€è½¬æ¢å¹¶æä¾›è¯¦ç»†çš„è¿è¡Œæ—¶æŒ‡æ ‡ã€‚ 
- å¯æŠ•å…¥ç”Ÿäº§çš„éƒ¨ç½²ï¼šåˆ©ç”¨ä¸“ä¸ºåº”å¯¹æœ‰çŠ¶æ€ã€é•¿æ—¶é—´è¿è¡Œçš„å·¥ä½œæµç¨‹æ‰€é¢ä¸´çš„ç‹¬ç‰¹æŒ‘æˆ˜è€Œè®¾è®¡çš„å¯æ‰©å±•åŸºç¡€è®¾æ–½ï¼Œè‡ªä¿¡åœ°éƒ¨ç½²å¤æ‚çš„æ™ºèƒ½ä½“ç³»ç»Ÿã€‚

## åˆå§‹åŒ–

å®‰è£…ï¼š

```
pip install -U langgraph
```

ç„¶åï¼Œä½¿ç”¨é¢„æ„å»ºç»„ä»¶åˆ›å»ºä¸€ä¸ªæ™ºèƒ½ä½“ï¼š

```
# å®‰è£…å¿…è¦åº“
# pip install dashscope langchain langchain-community langgraph

from langgraph.prebuilt import create_react_agent
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_core.messages import HumanMessage
from langchain.agents import Tool
import os
def get_weather(city: str) -> str:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯"""
    return f"{city}çš„å¤©æ°”æ˜¯æ™´å¤©ï¼Œ25â„ƒï¼"

# åˆ›å»ºåƒé—®æ¨¡å‹å®ä¾‹
model = ChatTongyi(
    model_name="qwen-turbo",   # ä¹Ÿå¯ä»¥ä½¿ç”¨ qwen-plus æˆ– qwen-max
    dashscope_api_key=os.getenv("DASHSCOPE_API_KEY") # æ›¿æ¢ä¸ºçœŸå®çš„APIå¯†é’¥
)

# åˆ›å»ºä»£ç†
agent = create_react_agent(
    model=model,
    tools=[Tool(
        name="get_weather",
        func=get_weather,
        description="è·å–åŸå¸‚çš„å¤©æ°”ä¿¡æ¯"
    )],
    prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„å¤©æ°”åŠ©æ‰‹"
)

# è¿è¡Œä»£ç†
response = agent.invoke({
    "messages": [
        HumanMessage(content="ä¸Šæµ·çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
    ]
})

# æ‰“å°å“åº”ç»“æœ
print("æœ€ç»ˆå›ç­”:", response["messages"][-1].content)
```

## å¿«é€Ÿå…¥é—¨

### StateGraph

é¦–å…ˆstategraphæ˜¯ç”¨æ¥æè¿°æ•´ä¸ªå›¾çš„ï¼Œå›¾ä¸­çš„çŠ¶æ€ä¼šéšç€å¤šä¸ªagentçš„å·¥ä½œä¸æ–­çš„æ›´æ–°ï¼ŒèŠ‚ç‚¹nodeå°±æ˜¯ç”¨æ¥æ›´æ–°çŠ¶æ€çš„å¦‚ä½•æ¥å®šä¹‰ä¸€å¼ å›¾ä¸­çš„çŠ¶æ€

```
from typing import Annotated

from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages


class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)#Stateå¿…é¡»ã€‚å®šä¹‰çŠ¶æ€æ•°æ®çš„ Pydantic æ¨¡å‹ï¼Œæè¿°å›¾ä¸­çš„æ•°æ®ç»“æ„
```

æˆ‘ä»¬çš„å›¾ç°åœ¨å¯ä»¥å¤„ç†ä¸¤ä¸ªå…³é”®ä»»åŠ¡ï¼š 

- æ¯ä¸ªnodeéƒ½å¯ä»¥æ¥æ”¶å½“å‰Stateä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºå¯¹è¯¥çŠ¶æ€çš„æ›´æ–°ã€‚ 
- ç”±äºä½¿ç”¨äº†å¸¦æœ‰Annotatedè¯­æ³•çš„é¢„æ„å»ºadd_messageså‡½æ•°ï¼Œå¯¹messagesçš„æ›´æ–°å°†è¿½åŠ åˆ°ç°æœ‰åˆ—è¡¨ä¸­ï¼Œè€Œä¸æ˜¯è¦†ç›–å®ƒã€‚

### Nodes

**Nodes** ä»£è¡¨å·¥ä½œå•å…ƒï¼Œé€šå¸¸æ˜¯æ™®é€šçš„ Python å‡½æ•°ã€‚

é¦–å…ˆé€‰æ‹©ä¸€ä¸ªæ¨¡å‹ï¼š

```
from langchain_community.chat_models.tongyi import ChatTongyi 
llm = ChatTongyi(
    model_name="qwen-turbo",  # ä¹Ÿå¯ä½¿ç”¨ qwen-plus æˆ– qwen-max
    dashscope_api_key=os.getenv("DASHSCOPE_API_KEY")  # æ›¿æ¢ä¸ºçœŸå®çš„APIå¯†é’¥
)
```

å°†èŠå¤©æ¨¡å‹æ•´åˆåˆ°ä¸€ä¸ªç®€å•çš„èŠ‚ç‚¹ä¸­ï¼š

```
def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}


# The first argument is the unique node name
# The second argument is the function or object that will be called whenever
# the node is used.
graph_builder.add_node("chatbot", chatbot)
```

chatbot èŠ‚ç‚¹å‡½æ•°æ¥æ”¶å½“å‰çš„ `State`ï¼ˆçŠ¶æ€ï¼‰ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸ã€‚è¿™ä¸ªå­—å…¸åŒ…å«ä¸€ä¸ªæ›´æ–°åçš„ `messages` åˆ—è¡¨ï¼Œè¯¥åˆ—è¡¨ä½äºé”® "messages" ä¹‹ä¸‹ã€‚è¿™å°±æ˜¯æ‰€æœ‰ LangGraph èŠ‚ç‚¹å‡½æ•°çš„åŸºæœ¬æ¨¡å¼ã€‚

 `State`ï¼ˆçŠ¶æ€ï¼‰ä¸­çš„ `add_messages` å‡½æ•°ä¼šå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å“åº”æ¶ˆæ¯é™„åŠ åˆ°çŠ¶æ€ä¸­å·²æœ‰çš„ä»»ä½•æ¶ˆæ¯ä¹‹åã€‚

### entry

æ·»åŠ ä¸€ä¸ªentryç‚¹ï¼Œä»¥ä¾¿æ¯æ¬¡è¿è¡Œå›¾æ—¶å‘ŠçŸ¥å›¾ä»ä½•å¤„å¼€å§‹å·¥ä½œï¼š

```
graph_builder.add_edge(START, "chatbot")
```

### Compile

åœ¨è¿è¡Œå›¾ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶è¿›è¡Œç¼–è¯‘ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨å›¾æ„å»ºå™¨ä¸Šè°ƒç”¨compile()æ¥å®ç°ã€‚è¿™å°†åˆ›å»ºä¸€ä¸ªCompiledGraphï¼Œæˆ‘ä»¬å¯ä»¥åœ¨çŠ¶æ€ä¸Šè°ƒç”¨å®ƒã€‚

```
graph = graph_builder.compile()
```

### å¯è§†åŒ–

```
from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass
```

![image-20250620171634220](https://s2.loli.net/2025/06/20/CoB3VsReNgDtSm4.png)

### è¿è¡Œ

```
def stream_graph_updates(user_input: str):
    for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)


while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break
        stream_graph_updates(user_input)
    except:
        # fallback if input() is not available
        user_input = "What do you know about LangGraph?"
        print("User: " + user_input)
        stream_graph_updates(user_input)
        break
```

>æµå¼è¾“å‡ºï¼š
>`stream()` æ–¹æ³•è¿”å›ä¸€ä¸ªå¯è¿­ä»£çš„ `event` æµï¼Œæ¯ä¸ª `event` ä»£è¡¨ç³»ç»Ÿå“åº”çš„ä¸€éƒ¨åˆ†ï¼ˆå¯èƒ½æ˜¯é€å­—ç”Ÿæˆçš„æ–‡æœ¬ï¼‰ã€‚é€šè¿‡ `for event in ...` å¾ªç¯ï¼Œå¯ä»¥é€æ¬¡å¤„ç†è¿™äº›æµå¼äº‹ä»¶ã€‚

## æ·»åŠ å·¥å…·

### å®šä¹‰æœç´¢å·¥å…·

```
from langchain.agents import AgentExecutor, Tool, ZeroShotAgent
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain_community.utilities import SerpAPIWrapper
from langchain_openai import ChatOpenAI

# å®šä¹‰Tool
# éœ€è¦å®šä¹‰ç¯å¢ƒå˜é‡ export GOOGLE_API_KEY="", åœ¨ç½‘ç«™ä¸Šæ³¨å†Œå¹¶ç”ŸæˆAPI Key: https://serpapi.com/searches

search = SerpAPIWrapper()
tools = [
    Tool(
        name="Search",
        func=search.run,
        description="useful for when you need to answer questions about current events",
    )
]
```

### å®šä¹‰è¯­è¨€æ¨¡å‹

```
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_core.messages import HumanMessage
from langchain.agents import Tool
import os

# åˆ›å»ºåƒé—®æ¨¡å‹å®ä¾‹
llm = ChatTongyi(
    model_name="qwen-turbo",   # ä¹Ÿå¯ä»¥ä½¿ç”¨ qwen-plus æˆ– qwen-max
    dashscope_api_key=os.getenv("DASHSCOPE_API_KEY") # æ›¿æ¢ä¸ºçœŸå®çš„APIå¯†é’¥
)
```

### å®šä¹‰å›¾

bind_toolsï¼šè¿™ä½¿å¾—å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çŸ¥é“å¦‚æœå®ƒæƒ³ä½¿ç”¨æœç´¢å¼•æ“ï¼Œåº”è¯¥ä½¿ç”¨çš„æ­£ç¡®JSONæ ¼å¼ã€‚

```
from typing import Annotated

from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

# Modification: tell the LLM which tools it can call
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
```

åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥è¿è¡Œå·¥å…·
ç°åœ¨ï¼Œå¦‚æœå·¥å…·è¢«è°ƒç”¨ï¼Œåˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥è¿è¡Œè¿™äº›å·¥å…·ã€‚å…·ä½“åšæ³•æ˜¯ï¼Œå°†è¿™äº›å·¥å…·æ·»åŠ åˆ°ä¸€ä¸ªåä¸ºBasicToolNodeçš„æ–°èŠ‚ç‚¹ä¸­ï¼Œè¯¥èŠ‚ç‚¹ä¼šæ£€æŸ¥çŠ¶æ€ä¸­æœ€æ–°çš„æ¶ˆæ¯ï¼Œå¦‚æœæ¶ˆæ¯ä¸­åŒ…å«tool_callsï¼Œå°±ä¼šè°ƒç”¨å·¥å…·ã€‚è¿™ä¾èµ–äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„tool_callingæ”¯æŒï¼ŒAnthropicã€OpenAIã€è°·æ­ŒGeminiä»¥åŠå…¶ä»–ä¸€äº›å¤§è¯­è¨€æ¨¡å‹æä¾›å•†éƒ½æä¾›è¿™ç§æ”¯æŒã€‚ 

```
import json

from langchain_core.messages import ToolMessage


class BasicToolNode:
    """A node that runs the tools requested in the last AIMessage."""

    def __init__(self, tools: list) -> None:
        self.tools_by_name = {tool.name: tool for tool in tools}

    def __call__(self, inputs: dict):
        if messages := inputs.get("messages", []):
            message = messages[-1]
        else:
            raise ValueError("No message found in input")
        outputs = []
        for tool_call in message.tool_calls:
            tool_result = self.tools_by_name[tool_call["name"]].invoke(
                tool_call["args"]
            )
            outputs.append(
                ToolMessage(
                    content=json.dumps(tool_result),
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            )
        return {"messages": outputs}


tool_node = BasicToolNode(tools=tools)
graph_builder.add_node("tools", tool_node)
```

ä¹Ÿå¯ä»¥ä½¿ç”¨LangGraphé¢„å…ˆæ„å»ºå¥½çš„ ToolNode

```
from typing import Annotated

from langchain_tavily import TavilySearch
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

tool = TavilySearch(max_results=2)
tools = [tool]
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)

tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)

graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
# Any time a tool is called, we return to the chatbot to decide the next step
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
graph = graph_builder.compile()
```

### å®šä¹‰æ¡ä»¶è¾¹

æ·»åŠ äº†å·¥å…·èŠ‚ç‚¹åï¼Œç°åœ¨ä½ å¯ä»¥å®šä¹‰æ¡ä»¶è¾¹äº†ã€‚

è¾¹å°†æ§åˆ¶æµä»ä¸€ä¸ªèŠ‚ç‚¹å¯¼å‘ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ã€‚æ¡ä»¶è¾¹ä»å•ä¸ªèŠ‚ç‚¹å‡ºå‘ï¼Œé€šå¸¸åŒ…å« â€œifâ€ è¯­å¥ï¼Œä»¥ä¾¿æ ¹æ®å½“å‰å›¾çŠ¶æ€å¯¼å‘ä¸åŒçš„èŠ‚ç‚¹ã€‚è¿™äº›å‡½æ•°æ¥æ”¶å½“å‰å›¾çŠ¶æ€ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ŒæŒ‡ç¤ºæ¥ä¸‹æ¥è¦è°ƒç”¨å“ªä¸ªï¼ˆå“ªäº›ï¼‰èŠ‚ç‚¹ã€‚

æ¥ä¸‹æ¥ï¼Œå®šä¹‰ä¸€ä¸ªåä¸º route_tools çš„è·¯ç”±å‡½æ•°ï¼Œç”¨äºæ£€æŸ¥èŠå¤©æœºå™¨äººè¾“å‡ºä¸­çš„å·¥å…·è°ƒç”¨ã€‚é€šè¿‡è°ƒç”¨ add_conditional_edges å°†æ­¤å‡½æ•°æä¾›ç»™å›¾ï¼Œè¿™ä¼šå‘Šè¯‰å›¾ï¼Œæ¯å½“èŠå¤©æœºå™¨äººèŠ‚ç‚¹å®Œæˆæ—¶ï¼Œæ£€æŸ¥æ­¤å‡½æ•°ä»¥ç¡®å®šæ¥ä¸‹æ¥çš„èµ°å‘ã€‚

è¯¥æ¡ä»¶åœ¨å­˜åœ¨å·¥å…·è°ƒç”¨æ—¶å°†å¯¼å‘å·¥å…·ï¼Œä¸å­˜åœ¨æ—¶åˆ™å¯¼å‘ ENDã€‚ç”±äºè¯¥æ¡ä»¶å¯ä»¥è¿”å› ENDï¼Œè¿™æ¬¡ä½ æ— éœ€æ˜¾å¼è®¾ç½® finish_pointã€‚

**add_conditional_edgeså‚æ•°**ï¼š

- sourceï¼šèµ·å§‹èŠ‚ç‚¹ã€‚å½“é€€å‡ºæ­¤èŠ‚ç‚¹æ—¶ï¼Œå°†è¿è¡Œæ­¤æ¡ä»¶è¾¹ã€‚
- pathï¼šå¯è°ƒç”¨å¯¹è±¡ï¼Œç”¨äºç¡®å®šä¸‹ä¸€ä¸ªèŠ‚ç‚¹æˆ–å¤šä¸ªèŠ‚ç‚¹ã€‚å¦‚æœæœªæŒ‡å®špath_mapï¼Œåˆ™è¯¥å¯è°ƒç”¨å¯¹è±¡åº”è¿”å›ä¸€ä¸ªæˆ–å¤šä¸ªèŠ‚ç‚¹åç§°ã€‚å¦‚æœè¿”å›ENDï¼Œå›¾å°†#åœæ­¢æ‰§è¡Œã€‚
- path_mapï¼šå¯é€‰çš„è·¯å¾„åˆ°èŠ‚ç‚¹åç§°çš„æ˜ å°„ã€‚å¦‚æœçœç•¥ï¼Œpathè¿”å›çš„è·¯å¾„åº”ç›´æ¥ä¸ºèŠ‚ç‚¹åç§°ã€‚

```
def route_tools(
    state: State,
):
    """
    Use in the conditional_edge to route to the ToolNode if the last message
    has tool calls. Otherwise, route to the end.
    """
    if isinstance(state, list):
        ai_message = state[-1]
    elif messages := state.get("messages", []):
        ai_message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")
    if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0:
        return "tools"
    return END


# The `tools_condition` function returns "tools" if the chatbot asks to use a tool, and "END" if
# it is fine directly responding. This conditional routing defines the main agent loop.
#add_conditional_edgeså‚æ•°ï¼š
#sourceï¼šèµ·å§‹èŠ‚ç‚¹ã€‚å½“é€€å‡ºæ­¤èŠ‚ç‚¹æ—¶ï¼Œå°†è¿è¡Œæ­¤æ¡ä»¶è¾¹ã€‚
#pathï¼šå¯è°ƒç”¨å¯¹è±¡ï¼Œç”¨äºç¡®å®šä¸‹ä¸€ä¸ªèŠ‚ç‚¹æˆ–å¤šä¸ªèŠ‚ç‚¹ã€‚å¦‚æœæœªæŒ‡å®špath_mapï¼Œåˆ™è¯¥å¯è°ƒç”¨å¯¹è±¡åº”è¿”å›ä¸€ä¸ªæˆ–å¤šä¸ªèŠ‚ç‚¹åç§°ã€‚å¦‚æœè¿”å›ENDï¼Œå›¾å°†#åœæ­¢æ‰§è¡Œã€‚
#path_mapï¼šå¯é€‰çš„è·¯å¾„åˆ°èŠ‚ç‚¹åç§°çš„æ˜ å°„ã€‚å¦‚æœçœç•¥ï¼Œpathè¿”å›çš„è·¯å¾„åº”ç›´æ¥ä¸ºèŠ‚ç‚¹åç§°ã€‚
graph_builder.add_conditional_edges(
    "chatbot",
    route_tools,
    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node
    # It defaults to the identity function, but if you
    # want to use a node named something else apart from "tools",
    # You can update the value of the dictionary to something else
    # e.g., "tools": "my_tools"
    {"tools": "tools", END: END},
)
# Any time a tool is called, we return to the chatbot to decide the next step
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
graph = graph_builder.compile()
```

å¾—åˆ°ä»¥ä¸‹æ¨¡å‹

![image-20250622203633854](https://s2.loli.net/2025/06/22/2gSKdGwWBPFUO4f.png)

### è¿è¡Œ

```
def stream_graph_updates(user_input: str):
    for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)

while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break

        stream_graph_updates(user_input)
    except:
        # fallback if input() is not available
        user_input = "What do you know about LangGraph?"
        print("User: " + user_input)
        stream_graph_updates(user_input)
        break
```

## æ·»åŠ è®°å¿†

LangGraph é€šè¿‡æŒä¹…åŒ–æ£€æŸ¥ç‚¹ä¿å­˜ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å¦‚æœä½ åœ¨ç¼–è¯‘å›¾æ—¶æä¾›ä¸€ä¸ªæ£€æŸ¥ç‚¹å·¥å…·ï¼Œå¹¶åœ¨è°ƒç”¨å›¾æ—¶æä¾›ä¸€ä¸ªçº¿ç¨‹ IDï¼ŒLangGraph ä¼šåœ¨æ¯ä¸€æ­¥ä¹‹åè‡ªåŠ¨ä¿å­˜çŠ¶æ€ã€‚å½“ä½ ä½¿ç”¨ç›¸åŒçš„çº¿ç¨‹ ID å†æ¬¡è°ƒç”¨å›¾æ—¶ï¼Œå›¾ä¼šåŠ è½½å…¶ä¿å­˜çš„çŠ¶æ€ï¼Œä½¿èŠå¤©æœºå™¨äººèƒ½å¤Ÿä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­ã€‚

### åˆ›å»º`MemorySaver`æ£€æŸ¥ç‚¹

åˆ›å»ºåŸºäºå†…å­˜çš„æ£€æŸ¥ç‚¹

```
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
```

### ç¼–è¯‘å›¾



```
graph = graph_builder.compile(checkpointer=memory)
```



### è¿è¡Œ

é€‰æ‹©ä¸€ä¸ªçº¿ç¨‹ä½œä¸ºæ­¤æ¬¡å¯¹è¯çš„keyï¼š

```
config = {"configurable": {"thread_id": "1"}}
```

è¿è¡Œå›¾ï¼š

```
user_input = "Hi there! My name is Will."

# The config is the **second positional argument** to stream() or invoke()!
events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    config,
    stream_mode="values",
)
for event in events:
    event["messages"][-1].pretty_print()
```

ç»§ç»­è¯¢é—®ï¼š

```
user_input = "Remember my name?"

# The config is the **second positional argument** to stream() or invoke()!
events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    config,
    stream_mode="values",
)
for event in events:
    event["messages"][-1].pretty_print()
```

### æ£€æŸ¥çŠ¶æ€

å¿«ç…§åŒ…å«å½“å‰çŠ¶æ€å€¼ã€ç›¸åº”çš„é…ç½®ä»¥åŠ`next`è¦å¤„ç†çš„èŠ‚ç‚¹ã€‚

```
snapshot = graph.get_state(config)
snapshot
```

> å¦‚æœåœ¨å›¾å½¢è°ƒç”¨ä¸­è·å–çŠ¶æ€ï¼Œ`snapshot.next` ä¼šå‘ŠçŸ¥ä¸‹ä¸€ä¸ªå°†æ‰§è¡Œçš„èŠ‚ç‚¹

## æ·»åŠ äººå·¥ä»‹å…¥æ§åˆ¶

æ™ºèƒ½ä½“å¯èƒ½ä¸å¯é ï¼Œå¯èƒ½éœ€è¦äººç±»è¾“å…¥æ‰èƒ½æˆåŠŸå®Œæˆä»»åŠ¡ã€‚åŒæ ·ï¼Œå¯¹äºæŸäº›æ“ä½œï¼Œä½ å¯èƒ½å¸Œæœ›åœ¨è¿è¡Œå‰è·å¾—äººç±»æ‰¹å‡†ï¼Œä»¥ç¡®ä¿ä¸€åˆ‡æŒ‰é¢„æœŸè¿è¡Œã€‚ LangGraphçš„æŒä¹…å±‚æ”¯æŒäººåœ¨å›è·¯å·¥ä½œæµç¨‹ï¼Œå…è®¸æ ¹æ®ç”¨æˆ·åé¦ˆæš‚åœå’Œæ¢å¤æ‰§è¡Œã€‚æ­¤åŠŸèƒ½çš„ä¸»è¦æ¥å£æ˜¯`interrupt`å‡½æ•°ã€‚åœ¨èŠ‚ç‚¹å†…éƒ¨è°ƒç”¨`interrupt`å°†æš‚åœæ‰§è¡Œã€‚é€šè¿‡ä¼ å…¥ä¸€ä¸ª`Command`ï¼Œå¯ä»¥è¿åŒæ¥è‡ªäººç±»çš„æ–°è¾“å…¥ä¸€èµ·æ¢å¤æ‰§è¡Œã€‚`interrupt`åœ¨ä½¿ç”¨ä¸Šç±»ä¼¼äºPythonçš„å†…ç½®å‡½æ•°`input()`ï¼Œä½†æœ‰ä¸€äº›æ³¨æ„äº‹é¡¹ã€‚ 

### æ·»åŠ `human_assistance`å·¥å…·

å®šä¹‰æ¨¡å‹ï¼š

```
from langchain_community.chat_models.tongyi import ChatTongyi  # æ›¿æ¢ä¸ºåƒé—®æ¨¡å‹
llm = ChatTongyi(
    model_name="qwen-turbo",  # ä¹Ÿå¯ä½¿ç”¨ qwen-plus æˆ– qwen-max
    dashscope_api_key=os.getenv("DASHSCOPE_API_KEY")  # æ›¿æ¢ä¸ºçœŸå®çš„APIå¯†é’¥
)
```

æ·»åŠ  `human_assistance` ï¼š

```
from typing import Annotated

from langchain_core.tools import tool, Tool
from typing_extensions import TypedDict

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

from langgraph.types import Command, interrupt

class State(TypedDict):
    messages: Annotated[list, add_messages]


from langchain_community.utilities import SerpAPIWrapper

# å®šä¹‰Tool
# éœ€è¦å®šä¹‰ç¯å¢ƒå˜é‡ export GOOGLE_API_KEY="", åœ¨ç½‘ç«™ä¸Šæ³¨å†Œå¹¶ç”ŸæˆAPI Key: https://serpapi.com/searches

# search = SerpAPIWrapper()
# tools = [
#     Tool(
#         name="Search",
#         func=search.run,
#         description="useful for when you need to answer questions about current events",
#     )
# ]
class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

@tool
def human_assistance(query: str) -> str:
    """Request assistance from a human."""
    human_response = interrupt({"query": query})
    return human_response["data"]
@tool
def Search(query: str) -> str:
    "Perform information retrieval based on user queries."
    tool = SerpAPIWrapper()
    return tool.run(query)
    
    
tools = [Search, human_assistance]
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    message = llm_with_tools.invoke(state["messages"])
    # Because we will be interrupting during tool execution,
    # we disable parallel tool calling to avoid repeating any
    # tool invocations when we resume.
    assert len(message.tool_calls) <= 1
    return {"messages": [message]}

graph_builder.add_node("chatbot", chatbot)

tool_node = ToolNode(tools=tools)
graph_builder.add_node("tools", tool_node)

graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
```

### ç¼–è¯‘å›¾

```
memory = MemorySaver()

graph = graph_builder.compile(checkpointer=memory)
```

### å‘èŠå¤©æœºå™¨äººæé—®

å‘èŠå¤©æœºå™¨äººæå‡ºä¸€ä¸ªèƒ½è°ƒç”¨çš„uman_assistanceå·¥å…·çš„é—®é¢˜ï¼š

```
user_input = "I need some expert guidance for building an AI agent. Could you request assistance for me?"
config = {"configurable": {"thread_id": "1"}}

events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    config,
    stream_mode="values",
)
for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()
```

è¾“å‡ºï¼š

```
================================[1m Human Message [0m=================================

I need some expert guidance for building an AI agent. Could you request assistance for me?
==================================[1m Ai Message [0m==================================
Tool Calls:
  human_assistance (call_8fded5dd235844db9ea6eb)
 Call ID: call_8fded5dd235844db9ea6eb
  Args:
    query: I need some expert guidance for building an AI agent.

```

 èŠå¤©æœºå™¨äººç”Ÿæˆäº†ä¸€ä¸ªå·¥å…·è°ƒç”¨ï¼Œä½†éšåæ‰§è¡Œè¢«ä¸­æ–­ã€‚å¦‚æœä½ æ£€æŸ¥å›¾çŠ¶æ€ï¼Œä¼šå‘ç°å®ƒåœ¨å·¥å…·èŠ‚ç‚¹å¤„åœæ­¢ï¼š

```
snapshot = graph.get_state(config)
snapshot.next
```

> ä¸Pythonçš„å†…ç½®input()å‡½æ•°ç±»ä¼¼ï¼Œåœ¨å·¥å…·å†…éƒ¨è°ƒç”¨interruptå°†æš‚åœæ‰§è¡Œã€‚è¿›åº¦ä¼šæ ¹æ®checkpointerè¿›è¡ŒæŒä¹…åŒ–ï¼›å› æ­¤ï¼Œå¦‚æœä½¿ç”¨Postgresè¿›è¡ŒæŒä¹…åŒ–ï¼Œåªè¦æ•°æ®åº“å¤„äºè¿è¡ŒçŠ¶æ€ï¼Œå°±å¯ä»¥éšæ—¶æ¢å¤æ‰§è¡Œã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œå®ƒä½¿ç”¨å†…å­˜ä¸­çš„checkpointerè¿›è¡ŒæŒä¹…åŒ–ï¼Œåªè¦Pythonå†…æ ¸æ­£åœ¨è¿è¡Œï¼Œå°±å¯ä»¥éšæ—¶æ¢å¤ã€‚ 

### æ¢å¤æ‰§è¡Œ

è¦æ¢å¤æ‰§è¡Œï¼Œè¯·ä¼ é€’ä¸€ä¸ªåŒ…å«å·¥å…·æ‰€éœ€æ•°æ®çš„Commandå¯¹è±¡ã€‚æ­¤æ•°æ®çš„æ ¼å¼å¯æ ¹æ®éœ€è¦è‡ªå®šä¹‰ã€‚åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œä½¿ç”¨ä¸€ä¸ªé”®ä¸º"data"çš„å­—å…¸ï¼š

```
human_response = (
    "We, the experts are here to help! We'd recommend you check out LangGraph to build your agent."
    " It's much more reliable and extensible than simple autonomous agents."
)

human_command = Command(resume={"data": human_response})

events = graph.stream(human_command, config, stream_mode="values")
for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()
```

## è‡ªå®šä¹‰çŠ¶æ€

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œå°†å‘çŠ¶æ€æ·»åŠ æ›´å¤šå­—æ®µï¼Œä»¥ä¾¿åœ¨ä¸ä¾èµ–æ¶ˆæ¯åˆ—è¡¨çš„æƒ…å†µä¸‹å®šä¹‰å¤æ‚è¡Œä¸ºã€‚èŠå¤©æœºå™¨äººå°†ä½¿ç”¨å…¶æœç´¢å·¥å…·æŸ¥æ‰¾ç‰¹å®šä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬å‘ç»™äººå·¥è¿›è¡Œå®¡æ ¸ã€‚

### å‘çŠ¶æ€æ·»åŠ é”®

 é€šè¿‡å‘çŠ¶æ€æ·»åŠ nameå’Œbirthdayé”®ï¼Œæ›´æ–°èŠå¤©æœºå™¨äººä»¥æŸ¥è¯¢å®ä½“çš„ç”Ÿæ—¥ï¼š

```
from typing import Annotated

from typing_extensions import TypedDict

from langgraph.graph.message import add_messages


class State(TypedDict):
    messages: Annotated[list, add_messages]
    name: str
    birthday: str
```

### æ›´æ–°å·¥å…·å†…éƒ¨çš„çŠ¶æ€

åœ¨`human_assistance`å·¥å…·å†…éƒ¨å¡«å……çŠ¶æ€é”®ã€‚è¿™ä½¿å¾—äººç±»å¯ä»¥åœ¨ä¿¡æ¯å­˜å‚¨åˆ°çŠ¶æ€ä¸­ä¹‹å‰å¯¹å…¶è¿›è¡Œå®¡æŸ¥ã€‚ä½¿ç”¨[`Command`](https://langchain-ai.github.io/langgraph/concepts/low_level/#using-inside-tools)ä»å·¥å…·å†…éƒ¨å‘å‡ºçŠ¶æ€æ›´æ–°ã€‚

```
from typing import Annotated
from langchain_core.messages import ToolMessage
from langchain_core.tools import InjectedToolCallId, tool
from langgraph.types import Command

@tool
def human_assistance(
    name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId]
) -> Command:
    """Request assistance from a human."""
    print(f"--- Entering human_assistance tool ---")
    print(f"Initial name: {name}, birthday: {birthday}")
    from  langgraph.types import  interrupt
    human_response = interrupt(
        {
            "question": "Is this correct?",
            "name": name,
            "birthday": birthday,
        },
    )
    print(f"Human response: {human_response}")

    if human_response.get("correct", "").lower().startswith("y"):
        verified_name = name
        verified_birthday = birthday
        response = "Correct"
        print("Human confirmed correctness.")
    else:
        verified_name = human_response.get("name", name)
        verified_birthday = human_response.get("birthday", birthday)
        response = f"Made a correction: {human_response}"
        print(f"Human made corrections. New name: {verified_name}, new birthday: {verified_birthday}")

    state_update = {
        "name": verified_name,
        "birthday": verified_birthday,
        "messages": [ToolMessage(response, tool_call_id=tool_call_id)],
    }
    print(f"State update being returned: {state_update}")
    print(f"--- Exiting human_assistance tool ---")
    return Command(update=state_update)
```

### ä½¿ç”¨èŠå¤©æœºå™¨äººæŸ¥è¯¢

æç¤ºèŠå¤©æœºå™¨äººæŸ¥æ‰¾LangGraphåº“çš„â€œè¯ç”Ÿæ—¥æœŸâ€ï¼Œå¹¶æŒ‡ç¤ºèŠå¤©æœºå™¨äººåœ¨è·å–æ‰€éœ€ä¿¡æ¯åè°ƒç”¨human_assistanceå·¥å…·ã€‚é€šè¿‡åœ¨å·¥å…·çš„å‚æ•°ä¸­è®¾ç½®nameå’Œbirthdayï¼Œä½ å¯ä»¥è¿«ä½¿èŠå¤©æœºå™¨äººä¸ºè¿™äº›å­—æ®µç”Ÿæˆå»ºè®®ã€‚

```
user_input = (
    "Can you look up when LangGraph was released? "
    "When you have the answer, use the human_assistance tool for review."
)
config = {"configurable": {"thread_id": "1"}}

events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    config,
    stream_mode="values",
)
for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()
```

è¾“å‡ºï¼š

```
================================[1m Human Message [0m=================================

Can you look up when LangGraph was released? When you have the answer, use the human_assistance tool for review.
==================================[1m Ai Message [0m==================================
Tool Calls:
  tavily_search (call_103316aa46a54bab99e235)
 Call ID: call_103316aa46a54bab99e235
  Args:
    query: When was LangGraph released?
=================================[1m Tool Message [0m=================================
Name: tavily_search

{"query": "When was LangGraph released?", "follow_up_questions": null, "answer": null, "images": [], "results": [{"title": "Releases Â· langchain-ai/langgraph - GitHub", "url": "https://github.com/langchain-ai/langgraph/releases", "content": "Releases Â· langchain-ai/langgraph Â· GitHub *   fix(langgraph): remove deprecated `output` usage in favor of `output_schema` (#5095) *   Remove Checkpoint.writes (#4822) *   Remove old checkpoint test fixtures (#4814) *   fix(langgraph): remove deprecated `output` usage in favor of `output_schema` (#5095) *   Remove support for node reading a single managed value *   Remove Checkpoint.writes (#4822) *   Remove Checkpoint.pending_sends (#4820) *   Remove old checkpoint test fixtures (#4814) Changes since checkpoint==2.0.26 *   langgraph-checkpoint 2.1.0 *   Preparation for 0.5 release: langgraph-checkpoint (#5124) *   Remove Checkpoint.writes *   Remove Checkpoint.pending_sends *   Remove Checkpoint.writes (#4822) *   Remove Checkpoint.pending_sends (#4820) *   Remove old checkpoint test fixtures (#4814) *   Remove postgres shallow checkpointer (#4813) *   Remove Checkpoint.writes *   Remove Checkpoint.pending_sends *   Remove old checkpoint test fixtures *   Remove postgres shallow checkpointer", "score": 0.98562, "raw_content": null}, {"title": "LangGraph - LangChain", "url": "https://www.langchain.com/langgraph", "content": "Design agent-driven user experiences with LangGraph Platform's APIs. Quickly deploy and scale your application with infrastructure built for agents. LangGraph sets the foundation for how we can build and scale AI workloads â€” from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.â€ LangGraph sets the foundation for how we can build and scale AI workloads â€” from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. LangGraph Platform is a service for deploying and scaling LangGraph applications, with an opinionated API for building agent UXs, plus an integrated developer studio.", "score": 0.98391, "raw_content": null}], "response_time": 1.65}
==================================[1m Ai Message [0m==================================
Tool Calls:
  human_assistance (call_0fb6aff5612246d9976f84)
 Call ID: call_0fb6aff5612246d9976f84
  Args:
    name: LangGraph
    birthday: 2023-10-01
--- Entering human_assistance tool ---
Initial name: LangGraph, birthday: 2023-10-01

```

### æ·»åŠ äººå·¥ä»‹å…¥

èŠå¤©æœºå™¨äººæœªèƒ½è¯†åˆ«å‡ºæ­£ç¡®æ—¥æœŸï¼Œå› æ­¤è¯·ä¸ºå…¶æä¾›ç›¸å…³ä¿¡æ¯ï¼š

```
human_command = Command(
    resume={
        "name": "LangGraph",
        "birthday": "Jan 17, 2024",
    },
)

events = graph.stream(human_command, config, stream_mode="values")
for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()
```

è¾“å‡ºï¼š

```
==================================[1m Ai Message [0m==================================
Tool Calls:
  human_assistance (call_0fb6aff5612246d9976f84)
 Call ID: call_0fb6aff5612246d9976f84
  Args:
    name: LangGraph
    birthday: 2023-10-01
--- Entering human_assistance tool ---
Initial name: LangGraph, birthday: 2023-10-01
Human response: {'name': 'LangGraph', 'birthday': 'Jan 17, 2024'}
Human made corrections. New name: LangGraph, new birthday: Jan 17, 2024
State update being returned: {'name': 'LangGraph', 'birthday': 'Jan 17, 2024', 'messages': [ToolMessage(content="Made a correction: {'name': 'LangGraph', 'birthday': 'Jan 17, 2024'}", tool_call_id='call_0fb6aff5612246d9976f84')]}
--- Exiting human_assistance tool ---
=================================[1m Tool Message [0m=================================
Name: human_assistance

Made a correction: {'name': 'LangGraph', 'birthday': 'Jan 17, 2024'}
==================================[1m Ai Message [0m==================================

The release date of LangGraph is January 17, 2024. Let me know if you need any further assistance!

```

## æ—¶é—´å›æº¯

åœ¨å…¸å‹çš„èŠå¤©æœºå™¨äººå·¥ä½œæµç¨‹ä¸­ï¼Œç”¨æˆ·ä¸æœºå™¨äººè¿›è¡Œä¸€æ¬¡æˆ–å¤šæ¬¡äº¤äº’ä»¥å®Œæˆä»»åŠ¡ã€‚è®°å¿†å’Œäººå·¥ä»‹å…¥å¯åœ¨å›¾çŠ¶æ€ä¸­è®¾ç½®æ£€æŸ¥ç‚¹å¹¶æ§åˆ¶æœªæ¥çš„å›å¤ã€‚ å¦‚æœå¸Œæœ›ç”¨æˆ·èƒ½å¤Ÿä»å…ˆå‰çš„å›å¤å¼€å§‹å¹¶æ¢ç´¢ä¸åŒçš„ç»“æœï¼Œæˆ–è€…ï¼Œå¸Œæœ›ç”¨æˆ·èƒ½å¤Ÿå›æº¯èŠå¤©æœºå™¨äººçš„å·¥ä½œä»¥çº æ­£é”™è¯¯æˆ–å°è¯•ä¸åŒçš„ç­–ç•¥å¯ä»¥ä½¿ç”¨LangGraphçš„å†…ç½®æ—¶é—´å›æº¯åŠŸèƒ½æ¥åˆ›å»ºæ­¤ç±»åŠŸèƒ½ã€‚

### å›é€€Graph

é€šè¿‡ä½¿ç”¨å›¾è¡¨çš„ `get_state_history` æ–¹æ³•è·å–æ£€æŸ¥ç‚¹æ¥å›é€€graphã€‚ç„¶åï¼Œä½ å¯ä»¥åœ¨è¿™ä¸ªå…ˆå‰çš„æ—¶é—´ç‚¹æ¢å¤æ‰§è¡Œã€‚

```

```

